{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTcTTDYzs3eT"
      },
      "source": [
        "# Dataset Loader\n",
        "\n",
        "The first part of the Open Bandit Pipeline (OBP) is the dataset loader. For the Open Bandit Dataset (OBD), the loader is `opb.dataset.OpenBanditDataset` ([docs](https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.dataset.real.html#obp.dataset.real.OpenBanditDataset)). \n",
        "\n",
        "As with many classes in the OBP, the dataset modules are implemented with [dataclasses](https://docs.python.org/3.7/library/dataclasses.html).\n",
        "\n",
        "The dataset module inherits from `obp.dataset.base.BaseRealBanditDatset` ([docs](https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.dataset.base.html#module-obp.dataset.base)) and should implement three methods:\n",
        "- `load_raw_data()`: Load an on-disk representation of the dataset into the module. Used during initialization.\n",
        "- `pre_process()`: Perform any preprocessing needed to transform the raw data representation into a final representation.\n",
        "- `obtain_batch_bandit_feedback()`: Return a dictionary containing (at least) keys: `[\"action\",\"position\",\"reward\",\"pscore\",\"context\",\"n_rounds\"]`\n",
        "\n",
        "It is also helpful if the dataset module exposes a property `len_list`, which is how many items the bandit shows the user at a time. Often the answer is 1, though in the case of OBD it's 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y2okvOD9sCaE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import obp\n",
        "from src.data.obp_dataset import MovieLensDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPG05Yi8tMUr",
        "outputId": "1f2c5af3-c92d-4bc4-8fd9-195340644b21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/bryan/ace784ad-85ec-4b04-91fc-978eed5d4b4d/drl-recsys/src/data/obp_dataset.py:54: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only\n",
            "  self.load_raw_data()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Finished data load\n",
            "----- Preprocessing dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/bryan/ace784ad-85ec-4b04-91fc-978eed5d4b4d/drl-recsys/src/data/obp_dataset.py:209: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  _df[\"item_id_history\"] = pd.concat(df_list)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preprocessing\n"
          ]
        }
      ],
      "source": [
        "dataset = MovieLensDataset(\n",
        "    data_path=os.path.join(os.getcwd(), \"data/\"), \n",
        "    embedding_network_weights_path=\"model/pmf/emb_50_ratio_0.800000_bs_1000_e_258_wd_0.100000_lr_0.000100_trained_pmf.pt\", \n",
        "    embedding_dim=50,\n",
        "    users_num=943,\n",
        "    items_num=1682,\n",
        "    state_size=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feedback dict:\n",
            "  n_rounds: <class 'int'>\n",
            "  n_actions: <class 'int'>\n",
            "  action: <class 'numpy.ndarray'>\n",
            "  position: <class 'numpy.ndarray'>\n",
            "  reward: <class 'numpy.ndarray'>\n",
            "  pscore: <class 'numpy.ndarray'>\n",
            "  context: <class 'numpy.ndarray'>\n",
            "  action_context: <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "bandit_feedback = dataset.obtain_batch_bandit_feedback()\n",
        "print(\"feedback dict:\")\n",
        "for key, value in bandit_feedback.items():\n",
        "    print(f\"  {key}: {type(value)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected reward for uniform random actions: 0.2651\n"
          ]
        }
      ],
      "source": [
        "exp_rand_reward = round(bandit_feedback[\"reward\"].mean(),4)\n",
        "print(f\"Expected reward for uniform random actions: {exp_rand_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Off-Policy Evaluation (OPE)\n",
        "\n",
        "The next step is OPE which attempts to estimate the performance of online bandit algorithms using the logged bandit feedback and ReplayMethod(RM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import obp\n",
        "from obp.ope import (\n",
        "    OffPolicyEvaluation, \n",
        "    RegressionModel,\n",
        "    InverseProbabilityWeighting as IPS,\n",
        "    SelfNormalizedInverseProbabilityWeighting as SNIPS,\n",
        "    DirectMethod as DM,\n",
        "    DoublyRobust as DR,\n",
        "    DoublyRobustWithShrinkage as DRos,\n",
        ")\n",
        "\n",
        "from src.model.simulator import run_bandit_simulation\n",
        "from src.model.bandit import EpsilonGreedy, LinUCB, WFairLinUCB, FairLinUCB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_groups = 10\n",
        "fairness_weight = {k: 1.0 for k in range(1, n_groups + 1)}\n",
        "with open(\"data/ml-100k/movies_groups.pkl\", \"rb\") as pkl_file:\n",
        "    movies_groups = pickle.load(pkl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch.utils.data.dataset import Dataset, random_split\n",
        "# class BanditDataSet(Dataset):\n",
        "#     def __init__(self, action, reward, position, context, rounds):\n",
        "#         self.action = action\n",
        "#         self.reward = reward\n",
        "#         self.position = position\n",
        "#         self.context = context\n",
        "#         self.rounds = rounds\n",
        "        \n",
        "#     def __getitem__(self, index):\n",
        "#         return (\n",
        "#             self.action[index], \n",
        "#             self.reward[index], \n",
        "#             self.position[index], \n",
        "#             self.context[index].reshape(1, 150)\n",
        "#         )\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.rounds\n",
        "\n",
        "# bandit_dataset = BanditDataSet(\n",
        "#     bandit_feedback[\"action\"],\n",
        "#     bandit_feedback[\"reward\"],\n",
        "#     bandit_feedback[\"position\"],\n",
        "#     bandit_feedback[\"context\"],\n",
        "#     bandit_feedback[\"n_rounds\"]\n",
        "# )\n",
        "\n",
        "# kwargs = {\"num_workers\": 1, \"pin_memory\": True} \n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     dataset=bandit_dataset, batch_size=1, shuffle=True, **kwargs\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 98266/98266 [00:07<00:00, 13185.69it/s]\n",
            "100%|██████████| 98266/98266 [00:06<00:00, 15916.70it/s]\n",
            "100%|██████████| 98266/98266 [00:06<00:00, 15274.81it/s]\n",
            "100%|██████████| 98266/98266 [00:06<00:00, 16058.42it/s]\n",
            "100%|██████████| 98266/98266 [00:06<00:00, 15547.60it/s]\n"
          ]
        }
      ],
      "source": [
        "epsilon_greedy = EpsilonGreedy(\n",
        "    n_actions=dataset.n_actions,\n",
        "    epsilon=0.1,\n",
        "    n_group=n_groups,\n",
        "    item_group=movies_groups,\n",
        "    fairness_weight=fairness_weight\n",
        ")\n",
        "eg_action_dist, eg_aligned_cvr, eg_cvr, eg_propfair, eg_ufg, eg_group_count = run_bandit_simulation(\n",
        "    bandit_feedback=bandit_feedback,\n",
        "    policy=epsilon_greedy,\n",
        "    epochs=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 3033/98266 [04:46<3:41:31,  7.16it/s]"
          ]
        }
      ],
      "source": [
        "lin_ucb = WFairLinUCB(\n",
        "    dim=dataset.dim_context,\n",
        "    n_actions=dataset.n_actions,\n",
        "    epsilon=0.25,\n",
        "    n_group=n_groups,\n",
        "    item_group=movies_groups,\n",
        "    fairness_weight=fairness_weight,\n",
        "    batch_size=1\n",
        ")\n",
        "linucb_action_dist, linucb_aligned_cvr, linucb_cvr, linucb_propfair, linucb_ufg, linucb_group_count = run_bandit_simulation(\n",
        "    bandit_feedback=bandit_feedback,\n",
        "    policy=lin_ucb,\n",
        "    epochs=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# estimate the policy value of the online bandit algorithms using RM\n",
        "ope = OffPolicyEvaluation(\n",
        "    bandit_feedback=bandit_feedback,\n",
        "    ope_estimators=[\n",
        "        IPS(estimator_name=\"IPS\"), \n",
        "        SNIPS(estimator_name=\"SNIPS\"),\n",
        "        DM(estimator_name=\"DM\"), \n",
        "        DR(estimator_name=\"DR\"),\n",
        "        DRos(estimator_name=\"DRos\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obp.ope.RegressionModel\n",
        "regression_model = RegressionModel(\n",
        "    n_actions=dataset.n_actions, # number of actions; |A|\n",
        "    len_list=dataset.len_list, # number of items in a recommendation list; K\n",
        "    base_model=LogisticRegression(C=100, max_iter=100000), \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estimated_rewards = regression_model.fit_predict(\n",
        "    context=bandit_feedback[\"context\"],\n",
        "    action=bandit_feedback[\"action\"],\n",
        "    reward=bandit_feedback[\"reward\"],\n",
        "    position=bandit_feedback[\"position\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eg_estimated_policy_value = ope.estimate_policy_values(\n",
        "    action_dist=eg_action_dist, # \\pi_e(a|x)\n",
        "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}\n",
        ")\n",
        "eg_estimated_policy_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linucb_estimated_policy_value = ope.estimate_policy_values(\n",
        "    action_dist=linucb_action_dist, # \\pi_e(a|x)\n",
        "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}\n",
        ")\n",
        "linucb_estimated_policy_value"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "OBP.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b77059a6c14ae276c4c5def6df34ab1b622e87be13d34e07b8df4cd38d5f2d42"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('recsysrl': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
