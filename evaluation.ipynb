{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "from src.environment.ml_env import OfflineEnv\n",
    "from src.environment.ml_fair_env import OfflineEnv as OfflineFairEnv\n",
    "from src.model.recommender import DRRAgent, FairRecAgent\n",
    "\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"data/ml-100k\"\n",
    "STATE_SIZE = 5"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1NmejTIFPtf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import json \n",
    "import pickle\n",
    "\n",
    "dataset_path = \"data/movie_lens_100k_output_path.json\"\n",
    "with open(dataset_path) as json_file:\n",
    "    _dataset_path = json.load(json_file)\n",
    "\n",
    "with open(_dataset_path[\"train_users_dict\"], \"rb\") as pkl_file:\n",
    "    train_users_dict = pickle.load(pkl_file)\n",
    "with open(_dataset_path[\"train_users_history_lens\"], \"rb\") as pkl_file:\n",
    "    train_users_history_lens = pickle.load(pkl_file)\n",
    "with open(_dataset_path[\"eval_users_dict\"], \"rb\") as pkl_file:\n",
    "    eval_users_dict = pickle.load(pkl_file)\n",
    "with open(_dataset_path[\"eval_users_history_lens\"], \"rb\") as pkl_file:\n",
    "    eval_users_history_lens = pickle.load(pkl_file)\n",
    "with open(_dataset_path[\"users_history_lens\"], \"rb\") as pkl_file:\n",
    "    users_history_lens = pickle.load(pkl_file)\n",
    "with open(_dataset_path[\"movies_id_to_movies\"], \"rb\") as pkl_file:\n",
    "    movies_id_to_movies = pickle.load(pkl_file)\n",
    "with open(_dataset_path[\"movies_groups\"], \"rb\") as pkl_file:\n",
    "    movies_groups = pickle.load(pkl_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "users_num = 943\n",
    "items_num = 1682"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training setting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_users_num = int(users_num * 0.8)\n",
    "train_items_num = items_num\n",
    "print(train_users_num, train_items_num)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "754 1682\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating setting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "eval_users_num = int(users_num * 0.2)\n",
    "eval_items_num = items_num\n",
    "print(eval_users_num, eval_items_num)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "188 1682\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evalutation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def evaluate(recommender, env, top_k=False):\n",
    "    # episodic reward\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    mean_precision = 0\n",
    "    mean_ndcg = 0\n",
    "    mean_cvr = 0\n",
    "    mean_propfair = 0\n",
    "    mean_ufg = 0\n",
    "    # Environment\n",
    "    user_id, items_ids, done = env.reset()\n",
    "    while not done:\n",
    "\n",
    "        # Observe current state & Find action\n",
    "        ## Embedding\n",
    "        user_eb = recommender.embedding_network.get_layer('user_embedding')(np.array(user_id))\n",
    "        items_eb = recommender.embedding_network.get_layer('movie_embedding')(np.array(items_ids))\n",
    "\n",
    "        ## SRM state\n",
    "        state = recommender.srm_ave([np.expand_dims(user_eb, axis=0), np.expand_dims(items_eb, axis=0)])\n",
    "        ## Action(ranking score) \n",
    "        action = recommender.actor.network(state)\n",
    "        ## Item \n",
    "        recommended_item = recommender.recommend_item(action, env.recommended_items, top_k=top_k)\n",
    "        # Calculate reward and observe new state (in env)\n",
    "        ## Step\n",
    "        next_items_ids, reward, done, _ = env.step(recommended_item, top_k=top_k)\n",
    "        if top_k:\n",
    "            correct_list = [1 if r > 0 else 0 for r in reward]\n",
    "            # ndcg\n",
    "            dcg, idcg = calculate_ndcg(correct_list, [1 for _ in range(len(reward))])\n",
    "            mean_ndcg += dcg/idcg\n",
    "            \n",
    "            #precision\n",
    "            correct_num = top_k-correct_list.count(0)\n",
    "            mean_precision += correct_num/top_k\n",
    "            \n",
    "        reward = np.sum(reward)\n",
    "        items_ids = next_items_ids\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        propfair = 0\n",
    "        for group in range(10):\n",
    "            _group = group + 1\n",
    "            if _group not in env.group_count:\n",
    "                env.group_count[_group] = 0\n",
    "\n",
    "            propfair += [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1][group] * math.log(\n",
    "                1 + (env.group_count[_group] / len(recommended_item))\n",
    "            )\n",
    "        \n",
    "        cvr = correct_num / len(recommended_item)\n",
    "        ufg = propfair / max(1 - cvr, 0.01)\n",
    "\n",
    "        mean_propfair += propfair\n",
    "        mean_cvr += cvr\n",
    "        mean_ufg += ufg\n",
    "    \n",
    "    return mean_precision/steps, mean_ndcg/steps, mean_propfair/steps, mean_cvr/steps, mean_ufg/steps\n",
    "\n",
    "def calculate_ndcg(rel, irel):\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    rel = [1 if r>0 else 0 for r in rel]\n",
    "    for i, (r, ir) in enumerate(zip(rel, irel)):\n",
    "        dcg += (r)/np.log2(i+2)\n",
    "        idcg += (ir)/np.log2(i+2)\n",
    "    return dcg, idcg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def evaluate_fair(recommender, env, top_k=False):\n",
    "    # episodic reward\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    mean_precision = 0\n",
    "    mean_ndcg = 0\n",
    "    mean_cvr = 0\n",
    "    mean_propfair = 0\n",
    "    mean_ufg = 0\n",
    "    # Environment\n",
    "    user_id, items_ids, done = env.reset()\n",
    "    while not done:\n",
    "\n",
    "        # Observe current state & Find action\n",
    "        ## Embedding\n",
    "        items_eb = recommender.embedding_network.get_layer(\"movie_embedding\")(\n",
    "            np.array(items_ids)\n",
    "        )\n",
    "\n",
    "        groups_eb = []\n",
    "        for items in items_ids:\n",
    "            groups_eb.append(\n",
    "                recommender.embedding_network.get_layer(\"movie_embedding\")(\n",
    "                    np.array(\n",
    "                        [\n",
    "                            k - 1\n",
    "                            for k, v in env.movies_groups.items()\n",
    "                            if v == env.movies_groups[items]\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fairness_allocation = []\n",
    "        for group in range(recommender.n_groups):\n",
    "            _group = group + 1\n",
    "            if _group not in env.group_count:\n",
    "                env.group_count[_group] = 0\n",
    "            fairness_allocation.append(\n",
    "                env.group_count[_group] / len(env.recommended_items)\n",
    "            )\n",
    "\n",
    "        ## SRM state\n",
    "        state = recommender.srm_ave(\n",
    "            [\n",
    "                np.expand_dims(items_eb, axis=0),\n",
    "                groups_eb,\n",
    "                np.expand_dims(fairness_allocation, axis=0),\n",
    "            ]\n",
    "        )\n",
    "        ## Action(ranking score) \n",
    "        action = recommender.actor.network(state)\n",
    "        ## Item \n",
    "        recommended_item = recommender.recommend_item(action, env.recommended_items, top_k=top_k)\n",
    "        # Calculate reward and observe new state (in env)\n",
    "        ## Step\n",
    "        next_items_ids, reward, done, _ = env.step(recommended_item, top_k=top_k)\n",
    "        if top_k:\n",
    "            correct_list = [1 if r > 0 else 0 for r in reward]\n",
    "            # ndcg\n",
    "            dcg, idcg = calculate_ndcg(correct_list, [1 for _ in range(len(reward))])\n",
    "            mean_ndcg += dcg/idcg\n",
    "            \n",
    "            #precision\n",
    "            correct_num = top_k-correct_list.count(0)\n",
    "            mean_precision += correct_num/top_k\n",
    "            \n",
    "        reward = np.sum(reward)\n",
    "        items_ids = next_items_ids\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        propfair = 0\n",
    "        for group in range(10):\n",
    "            _group = group + 1\n",
    "            if _group not in env.group_count:\n",
    "                env.group_count[_group] = 0\n",
    "\n",
    "            propfair += [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1][group] * math.log(\n",
    "                1 + (env.group_count[_group] / len(recommended_item))\n",
    "            )\n",
    "        \n",
    "        cvr = correct_num / len(recommended_item)\n",
    "        ufg = propfair / max(1 - cvr, 0.01)\n",
    "\n",
    "        mean_propfair += propfair\n",
    "        mean_cvr += cvr\n",
    "        mean_ufg += ufg\n",
    "    \n",
    "    return mean_precision/steps, mean_ndcg/steps, mean_propfair/steps, mean_cvr/steps, mean_ufg/steps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sum_precision = 0\n",
    "sum_ndcg = 0\n",
    "sum_propfair = 0\n",
    "sum_cvr = 0\n",
    "sum_ufg = 0\n",
    "TOP_K = 10\n",
    "\n",
    "for user_id in eval_users_dict.keys():\n",
    "    env = OfflineFairEnv(# OfflineEnv( # OfflineFairEnv(\n",
    "        eval_users_dict, \n",
    "        users_history_lens, \n",
    "        movies_id_to_movies, \n",
    "        movies_groups, \n",
    "        STATE_SIZE, \n",
    "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "        fix_user_id=user_id\n",
    "    )\n",
    "    recommender = FairRecAgent( # DRRAgent( # FairRecAgent(\n",
    "        env, \n",
    "        users_num, \n",
    "        items_num, \n",
    "        STATE_SIZE,\n",
    "        6,# 3, # 6,\n",
    "        \"model/movie_lens_100k_fair/\", # \"model/movie_lens_100k/\", # \"model/movie_lens_100k_fair/\",\n",
    "        \"model/movie_lens_100k_fair/user_movie_at_once.h5\", # \"model/movie_lens_100k/user_movie_at_once.h5\", # \"model/movie_lens_100k_fair/user_movie_at_once.h5\",\n",
    "        \"movie_lens\",\n",
    "        True,\n",
    "        False,\n",
    "        50, \n",
    "        128, # 128, # 512,\n",
    "        0.001,\n",
    "        128, # 128, # 512,\n",
    "        0.001,\n",
    "        0.9,\n",
    "        0.001,\n",
    "        1000000,\n",
    "        32, # 32, # 64,\n",
    "        10,\n",
    "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "    )\n",
    "    recommender.actor.build_networks()\n",
    "    recommender.critic.build_networks()\n",
    "    recommender.load_model('model/movie_lens_100k_fair/actor_15000.h5', \n",
    "                           'model/movie_lens_100k_fair/critic_15000.h5')\n",
    "    precision, ndcg, propfair, cvr, ufg = evaluate_fair(recommender, env, top_k=TOP_K)\n",
    "    sum_precision += precision\n",
    "    sum_ndcg += ndcg\n",
    "    sum_propfair += propfair\n",
    "    sum_ufg += ufg\n",
    "    sum_cvr += cvr\n",
    "    \n",
    "print(f'precision@{TOP_K} : {round(sum_precision/len(eval_users_dict), 4)}, ndcg@{TOP_K} : {round(sum_ndcg/len(eval_users_dict), 4)}')\n",
    "print(f'PropFair : {round(sum_propfair/len(eval_users_dict), 4)}, CVR : {round(sum_cvr/len(eval_users_dict), 4)}, UFG : {round(sum_ufg/len(eval_users_dict), 4)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "precision@10 : 0.4322, ndcg@10 : 0.4182\n",
      "PropFair : 0.2479, CVR : 0.4322, UFG : 1.1064\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "RL_ActorCritic_DDPG_Movie_Recommendation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('recsysrl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "dd2c8efcc0287468cbdc8d48ef7ab2401bf84f5d5031f37633db7ca0dfd74379"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}