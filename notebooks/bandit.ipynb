{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "#Dependencies\n",
    "import os\n",
    "import json \n",
    "import pickle\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import recmetrics as rm\n",
    "from random import sample\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly_express as px\n",
    "\n",
    "\n",
    "from src.model.pmf import PMF\n",
    "from src.environment import OfflineEnv\n",
    "from src.model.recommender.bandit import LinUCB, FairLinUCB\n",
    "from src.recsys_fair_metrics.recsys_fair import RecsysFair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT = dict(linucb=LinUCB, fair_linucb = FairLinUCB)\n",
    "ENV = dict(linucb=OfflineEnv, fair_linucb=OfflineEnv)\n",
    "\n",
    "ALGORITHM = \"linucb\"\n",
    "N_GROUPS = 10\n",
    "STATE_SIZE = 5\n",
    "DONE_COUNT = 10\n",
    "FAIRNESS_CONSTRAINTS = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "REWARD_VERSION = \"paper\"\n",
    "REWARD_THRESHOLD = 4\n",
    "USER_INTENT_THRESHOLD = 0\n",
    "USER_INTENT = \"none\"\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_EPISODE_NUM = 10000\n",
    "\n",
    "DATASET_PATH = \"../data/yahoo_output_path.json\"\n",
    "USERS_NUM = 15400\n",
    "ITEMS_NUM = 1000\n",
    "EMBEDDING_NETWORK_WEIGHTS_PATH = \"../model/pmf/yahoo_emb_100_ratio_0.800000_bs_100000_e_59_r0.585930_wd_0.100000_lr_0.000100_trained_pmf.pt\"\n",
    "\n",
    "# DATASET_PATH = \"../data/movie_lens_100k_output_path.json\"\n",
    "# USERS_NUM = 943\n",
    "# ITEMS_NUM = 1682\n",
    "# EMBEDDING_NETWORK_WEIGHTS_PATH =  \"../model/pmf/ml_100k_emb_100_ratio_0.800000_bs_1000_e_200_wd_0.100000_lr_0.000100_trained_pmf.pt\"\n",
    "\n",
    "SAVE_PATH = \"../model/{}/\".format(ALGORITHM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_PATH) as json_file:\n",
    "    _dataset_path = json.load(json_file)\n",
    "\n",
    "dataset = {}\n",
    "with open(os.path.join(\"..\", _dataset_path[\"train_users_dict\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"train_users_dict\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"eval_users_dict\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"eval_users_dict\"] = pickle.load(pkl_file)\n",
    "\n",
    "# with open(os.path.join(\"..\", _dataset_path[\"item_groups\"]), \"rb\") as pkl_file:\n",
    "#     dataset[\"item_groups\"] = pickle.load(pkl_file)\n",
    "\n",
    "dataset[\"ratings_df\"] = pd.read_csv(os.path.join(\"..\", _dataset_path[\"ratings_df\"]))\n",
    "dataset[\"items_df\"] = pd.read_csv(os.path.join(\"..\", _dataset_path[\"items_df\"]))\n",
    "dataset[\"items_metadata\"] = pd.read_csv(os.path.join(\"..\", _dataset_path[\"items_metadata\"]))\n",
    "dataset[\"title_emb\"] = os.path.join(\"..\",_dataset_path[\"title_emb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_groups_paths = [\"model/yahoo/yahoo_2022-10-04_11-39-07/item_groups.pkl\"]\n",
    "\n",
    "with open(os.path.join(\"..\", item_groups_paths[0]), \"rb\") as pkl_file:\n",
    "    dataset[\"item_groups\"] = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AGENT[ALGORITHM](\n",
    "    epsilon = 0.1,\n",
    "    dim=5,\n",
    "    n_actions=ITEMS_NUM,\n",
    "    len_list=1\n",
    ")\n",
    "\n",
    "SAVE_PATH = \"../model/{}/{}\".format(ALGORITHM, agent.policy_name)\n",
    "os.makedirs(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = PMF(USERS_NUM, ITEMS_NUM, EMBEDDING_DIM).to(\"cuda\")\n",
    "reward_model.load_state_dict(\n",
    "    torch.load(\n",
    "        EMBEDDING_NETWORK_WEIGHTS_PATH,\n",
    "        map_location=torch.device(\"cuda\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "user_embeddings = reward_model.user_embeddings.weight.data\n",
    "item_embeddings = reward_model.item_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_emb(_items_ids):\n",
    "    items_eb = item_embeddings[_items_ids]\n",
    "\n",
    "    return items_eb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ENV[ALGORITHM](\n",
    "    users_dict=dataset[\"train_users_dict\"],\n",
    "    n_groups=N_GROUPS,\n",
    "    item_groups=dataset[\"item_groups\"],\n",
    "    items_metadata=dataset[\"items_metadata\"],\n",
    "    items_df=dataset[\"items_df\"],\n",
    "    state_size=STATE_SIZE,\n",
    "    done_count=DONE_COUNT,\n",
    "    fairness_constraints=FAIRNESS_CONSTRAINTS,\n",
    "    reward_threshold=REWARD_THRESHOLD,\n",
    "    reward_version=REWARD_VERSION,\n",
    "    user_intent_threshold=USER_INTENT_THRESHOLD,\n",
    "    user_intent=USER_INTENT,\n",
    "    title_emb_path=dataset[\"title_emb\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_model = reward_model\n",
    "env.item_embeddings = item_embeddings\n",
    "env.device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_precision = 0\n",
    "sum_ndcg = 0\n",
    "sum_propfair = 0\n",
    "sum_reward = 0\n",
    "\n",
    "for episode in tqdm(range(MAX_EPISODE_NUM)):\n",
    "    # episodic reward\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    critic_loss = 0\n",
    "    actor_loss = 0\n",
    "    mean_action = 0\n",
    "    mean_precision = 0\n",
    "    mean_ndcg = 0\n",
    "\n",
    "    list_recommended_item = []\n",
    "\n",
    "    # environment\n",
    "    user_id, items_ids, done = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # observe current state & Find action\n",
    "        group_counts = env.get_group_count()\n",
    "        state = np.array([items_ids])\n",
    "\n",
    "        ## action\n",
    "        recommended_item = agent.select_action(state)[0]\n",
    "        list_recommended_item.append(recommended_item)\n",
    "\n",
    "        next_items_ids, reward, done, info = env.step(\n",
    "            recommended_item, top_k=False\n",
    "        )\n",
    "\n",
    "        agent.update_params(recommended_item, reward, state)\n",
    "\n",
    "        items_ids = next_items_ids\n",
    "        episode_reward += np.sum(reward) if False else reward\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        mean_precision += info[\"precision\"]\n",
    "\n",
    "        if done:\n",
    "            propfair = 0\n",
    "            total_exp = np.sum(env.get_group_count())\n",
    "            if total_exp > 0:\n",
    "                propfair = np.sum(\n",
    "                    np.array(FAIRNESS_CONSTRAINTS)\n",
    "                    * np.log(\n",
    "                        1 + np.array(env.get_group_count()) / total_exp\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            sum_precision += mean_precision / steps\n",
    "            sum_ndcg += mean_ndcg / steps\n",
    "            sum_propfair += propfair\n",
    "            sum_reward += episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SAVE_PATH, \"{}.pkl\".format(agent.policy_name)), \"wb\") as file:\n",
    "    pickle.dump(agent, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_groups_df = pd.DataFrame(\n",
    "    dataset[\"item_groups\"].items(), columns=[\"item_id\", \"group\"]\n",
    ")\n",
    "catalog = item_groups_df.item_id.unique().tolist()\n",
    "\n",
    "top_k = [10]\n",
    "\n",
    "_precision = []\n",
    "_propfair = []\n",
    "_ufg = []\n",
    "_recommended_item = []\n",
    "_random_recommended_item = []\n",
    "_exposure = []\n",
    "for k in top_k:\n",
    "    sum_precision = 0\n",
    "    sum_propfair = 0\n",
    "    sum_reward = 0\n",
    "\n",
    "    recommended_item = []\n",
    "    random_recommended_item = []\n",
    "    exposure = []\n",
    "\n",
    "    env = ENV[ALGORITHM](\n",
    "        users_dict=dataset[\"eval_users_dict\"],\n",
    "        n_groups=N_GROUPS,\n",
    "        item_groups=dataset[\"item_groups\"],\n",
    "        items_metadata=dataset[\"items_metadata\"],\n",
    "        items_df=dataset[\"items_df\"],\n",
    "        state_size=STATE_SIZE,\n",
    "        done_count=k,\n",
    "        fairness_constraints=FAIRNESS_CONSTRAINTS,\n",
    "        reward_threshold=REWARD_THRESHOLD,\n",
    "        reward_version=REWARD_VERSION,\n",
    "        use_only_reward_model=True,\n",
    "        user_intent_threshold=USER_INTENT_THRESHOLD,\n",
    "        user_intent=USER_INTENT,\n",
    "        title_emb_path=dataset[\"title_emb\"],\n",
    "    )\n",
    "    available_users = env.available_users\n",
    "\n",
    "    for user_id in tqdm(available_users):\n",
    "\n",
    "\n",
    "        eval_env = ENV[ALGORITHM](\n",
    "            users_dict=dataset[\"eval_users_dict\"],\n",
    "            n_groups=N_GROUPS,\n",
    "            item_groups=dataset[\"item_groups\"],\n",
    "            items_metadata=dataset[\"items_metadata\"],\n",
    "            items_df=dataset[\"items_df\"],\n",
    "            state_size=STATE_SIZE,\n",
    "            done_count=k,\n",
    "            fairness_constraints=FAIRNESS_CONSTRAINTS,\n",
    "            reward_threshold=REWARD_THRESHOLD,\n",
    "            reward_version=REWARD_VERSION,\n",
    "            user_intent_threshold=USER_INTENT_THRESHOLD,\n",
    "            user_intent=USER_INTENT,\n",
    "            use_only_reward_model=True,\n",
    "            reward_model=reward_model,\n",
    "            device=\"cuda\",\n",
    "            fix_user_id=user_id,\n",
    "            title_emb_path=dataset[\"title_emb\"],\n",
    "        ) \n",
    "\n",
    "        steps = 0\n",
    "        mean_precision = 0\n",
    "        mean_ndcg = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        critic_loss = 0\n",
    "        actor_loss = 0\n",
    "\n",
    "        list_recommended_item = []\n",
    "\n",
    "        # Environment\n",
    "        user_id, items_ids, done = eval_env.reset()\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            # observe current state & Find action\n",
    "            state = np.array([items_ids])\n",
    "            action = agent.select_action(state)[0]\n",
    "            list_recommended_item.append(action)\n",
    "\n",
    "            # Calculate reward and observe new state (in env)\n",
    "            ## Step\n",
    "            next_items_ids, reward, done, info = eval_env.step(action)\n",
    "            # agent.update_params(action, reward, state)\n",
    "\n",
    "            items_ids = next_items_ids\n",
    "            episode_reward += np.sum(reward) if top_k else reward\n",
    "            steps += 1\n",
    "\n",
    "            mean_precision += info[\"precision\"]\n",
    "\n",
    "        propfair = 0\n",
    "        total_exp = np.sum(eval_env.get_group_count())\n",
    "        if total_exp > 0:\n",
    "            propfair = np.sum(\n",
    "                np.array(FAIRNESS_CONSTRAINTS)\n",
    "                * np.log(1 + np.array(eval_env.get_group_count()) / total_exp)\n",
    "            )\n",
    "\n",
    "        result = {\n",
    "            \"precision\": mean_precision / steps,\n",
    "            \"propfair\": propfair,\n",
    "            \"reward\": episode_reward,\n",
    "            \"recommended_items\": {user_id: list_recommended_item},\n",
    "            \"exposure\": (np.array(eval_env.get_group_count()) / total_exp).tolist(),\n",
    "            \"critic_loss\": critic_loss / steps,\n",
    "            \"actor_loss\": actor_loss / steps,\n",
    "        }\n",
    "\n",
    "        recommended_item.append(result[\"recommended_items\"])\n",
    "        random_recommended_item.append({user_id: sample(catalog, k)})\n",
    "        exposure.append(result[\"exposure\"])\n",
    "\n",
    "        sum_precision += result[\"precision\"]\n",
    "        sum_propfair += result[\"propfair\"]\n",
    "        sum_reward += result[\"reward\"]\n",
    "\n",
    "        del eval_env\n",
    "\n",
    "    _precision.append(sum_precision / len(dataset[\"eval_users_dict\"]))\n",
    "    _propfair.append(sum_propfair / len(dataset[\"eval_users_dict\"]))\n",
    "    _ufg.append(\n",
    "        (sum_propfair / len(dataset[\"eval_users_dict\"]))\n",
    "        / (1 - (sum_precision / len(dataset[\"eval_users_dict\"])))\n",
    "    )\n",
    "    _recommended_item.append(recommended_item)\n",
    "    _random_recommended_item.append(random_recommended_item)\n",
    "    _exposure.append(exposure)\n",
    "\n",
    "feature_df = pd.DataFrame(\n",
    "    item_groups_df[[\"item_id\"]]\n",
    "    .apply(lambda x: get_items_emb(x).cpu().numpy().tolist())[\n",
    "        \"item_id\"\n",
    "    ]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for k in range(len(top_k)):\n",
    "    recs = pd.DataFrame(\n",
    "        [list(i.values()) for i in _recommended_item[k]], columns=[\"sorted_actions\"]\n",
    "    )\n",
    "\n",
    "    exposure = np.array(_exposure[k]).mean(axis=0)\n",
    "    ideal_exposure = np.array(FAIRNESS_CONSTRAINTS) / np.sum(\n",
    "        FAIRNESS_CONSTRAINTS\n",
    "    )\n",
    "\n",
    "    metrics[top_k[k]] = {\n",
    "        \"precision\": round(_precision[k] * 100, 4),\n",
    "        \"propfair\": round(_propfair[k] * 100, 4),\n",
    "        \"ufg\": round(_ufg[k], 4),\n",
    "        \"exposure\": exposure.tolist(),\n",
    "        \"ideal_exposure\": ideal_exposure.tolist(),\n",
    "    }\n",
    "\n",
    "    # fig = go.Figure()\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=[i for i in range(1, N_GROUPS + 1)],\n",
    "    #         y=exposure,\n",
    "    #         mode=\"lines+markers\",\n",
    "    #         name=\"Group Exposure\",\n",
    "    #     )\n",
    "    # )\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=[i for i in range(1, N_GROUPS + 1)],\n",
    "    #         y=ideal_exposure,\n",
    "    #         mode=\"lines+markers\",\n",
    "    #         name=\"Ideal Exposure\",\n",
    "    #     )\n",
    "    # )\n",
    "    # fig.update_layout(\n",
    "    #     title=\"Group Exposure vs Ideal Exposure\",\n",
    "    #     xaxis_title=\"Group\",\n",
    "    #     yaxis_title=\"Exposure\",\n",
    "    # )\n",
    "    # fig.write_image(\n",
    "    #     os.path.join(\n",
    "    #         SAVE_PATH,\n",
    "    #         \"group_exposure_vs_ideal_exposure_{}.png\".format(k),\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # recs[\"user_id\"] = [list(i.keys())[0] for i in _recommended_item[k]]\n",
    "    # recsys_fair = RecsysFair(\n",
    "    #     df=recs,\n",
    "    #     supp_metadata=item_groups_df,\n",
    "    #     user_column=\"user_id\",\n",
    "    #     item_column=\"item_id\",\n",
    "    #     reclist_column=\"sorted_actions\",\n",
    "    # )\n",
    "\n",
    "    # fair_column = \"group\"\n",
    "    # ex = recsys_fair.exposure(fair_column, top_k[k])\n",
    "\n",
    "    # fig = ex.show(kind=\"per_group_norm\", column=fair_column)\n",
    "    # fig.write_image(\n",
    "    #     os.path.join(\n",
    "    #         SAVE_PATH, \"exposure_per_group_k{}.png\".format(top_k[k])\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # fig = ex.show(kind=\"per_rank_pos\", column=fair_column)\n",
    "    # fig.write_image(\n",
    "    #     os.path.join(\n",
    "    #         SAVE_PATH, \"exposure_per_rank_k{}.png\".format(top_k[k])\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # recs.to_csv(\n",
    "    #     os.path.join(\n",
    "    #         SAVE_PATH, \"recommended_item_k{}.csv\".format(top_k[k])\n",
    "    #     )\n",
    "    # )\n",
    "    # item_groups_df.to_csv(\n",
    "    #     os.path.join(SAVE_PATH, \"supp_metadata_k{}.csv\".format(top_k[k]))\n",
    "    # )\n",
    "\n",
    "with open(os.path.join(SAVE_PATH, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f)\n",
    "\n",
    "print(\"---------- Finish Evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rsrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23b1c047fd19bf20d4111f807c9e6bee9fdce0e396893bab828ebbcdf9a68cad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
