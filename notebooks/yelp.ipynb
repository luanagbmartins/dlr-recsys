{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pandas = []\n",
    "\n",
    "with open(\"../data/yelp/yelp_academic_dataset_business.json\", \"r\") as f:\n",
    "    reader = pd.read_json(f, orient=\"records\", lines=True, \n",
    "                          chunksize=1000)\n",
    "        \n",
    "    for chunk in reader:\n",
    "        reduced_chunk = chunk.query(\"`is_open` >= 1 & review_count >= 20\")\n",
    "        b_pandas.append(reduced_chunk)\n",
    "    \n",
    "df_item = pd.concat(b_pandas, ignore_index=True)\n",
    "del b_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item[[\"business_id\", \"state\"]].groupby(\"state\").count().sort_values(by=\"business_id\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pandas = []\n",
    "r_dtypes = {\n",
    "    \"stars\": np.float16, \n",
    "    \"useful\": np.int32, \n",
    "    \"funny\": np.int32,\n",
    "    \"cool\": np.int32,\n",
    "}\n",
    "with open(\"../data/yelp/yelp_academic_dataset_user.json\", \"r\") as f:\n",
    "    reader = pd.read_json(f, orient=\"records\", lines=True, \n",
    "                          dtype=r_dtypes, chunksize=1000)\n",
    "        \n",
    "    for chunk in reader:\n",
    "        reduced_chunk = chunk.drop(columns=[\n",
    "            \"name\", \"yelping_since\", \"friends\", \"useful\", \"funny\", \"cool\", \"fans\", \"elite\", \"average_stars\", \n",
    "            \"compliment_hot\", \"compliment_more\", \"compliment_profile\", \"compliment_cute\", \"compliment_list\", \n",
    "            \"compliment_note\", \"compliment_plain\", \"compliment_cool\", \"compliment_funny\", \"compliment_writer\", \"compliment_photos\"\n",
    "            ])\\\n",
    "                             .query(\"`review_count` >= 5\")\n",
    "        b_pandas.append(reduced_chunk)\n",
    "    \n",
    "df_user = pd.concat(b_pandas, ignore_index=True)\n",
    "del b_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pandas = []\n",
    "r_dtypes = {\n",
    "    \"stars\": np.float16, \n",
    "    \"useful\": np.int32, \n",
    "    \"funny\": np.int32,\n",
    "    \"cool\": np.int32,\n",
    "}\n",
    "with open(\"../data/yelp/yelp_academic_dataset_review.json\", \"r\") as f:\n",
    "    reader = pd.read_json(f, orient=\"records\", lines=True, \n",
    "                          dtype=r_dtypes, chunksize=1000)\n",
    "        \n",
    "    for chunk in reader:\n",
    "        reduced_chunk = chunk.drop(columns=[\"useful\", \"funny\", \"cool\", \"text\"])#\\\n",
    "                            #.query(\"`date` >= '2019-06-01'\")\n",
    "        b_pandas.append(reduced_chunk)\n",
    "    \n",
    "df = pd.concat(b_pandas, ignore_index=True)\n",
    "del b_pandas\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_user, how='inner', on='user_id')\n",
    "\n",
    "del df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item = df_item[df_item[\"business_id\"].isin(df[\"business_id\"].unique())]\n",
    "df = pd.merge(df, df_item, how='inner', on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"review_id\", \"state\"]].groupby(\"state\").count().sort_values(by=\"review_id\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"review_id\", \"city\"]].groupby(\"city\").count().sort_values(by=\"review_id\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"date\") \n",
    "df = df.rename(columns={\"business_id\": \"item_id\"})\n",
    "df_item = df_item.rename(columns={\"business_id\": \"item_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ca = df[df[\"state\"] == \"CA\"]\n",
    "# df_tn = df[df[\"state\"] == \"TN\"]\n",
    "# df_fl = df[df[\"state\"] == \"FL\"]\n",
    "# df_pa = df[df[\"state\"] == \"PA\"]\n",
    "# df_pd = df[df[\"city\"] == \"Philadelphia\"]\n",
    "\n",
    "# del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_item_ca = df_item[df_item[\"state\"] == \"CA\"]\n",
    "# df_item_tn = df_item[df_item[\"state\"] == \"TN\"]\n",
    "# df_item_fl = df_item[df_item[\"state\"] == \"FL\"]\n",
    "# df_item_pa = df_item[df_item[\"state\"] == \"PA\"]\n",
    "# df_item_pd = df_item[df_item[\"city\"] == \"Philadelphia\"]\n",
    "\n",
    "# del df_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_encoder = preprocessing.LabelEncoder().fit(df.item_id.values)\n",
    "df.item_id = item_encoder.transform(df.item_id.values)\n",
    "df_item.item_id = item_encoder.transform(df_item.item_id.values)\n",
    "\n",
    "\n",
    "user_encoder = preprocessing.LabelEncoder().fit(df.user_id.values)\n",
    "df.user_id = user_encoder.transform(df.user_id.values)\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# item_encoder = preprocessing.LabelEncoder().fit(df_ca.item_id.values)\n",
    "# df_ca.item_id = item_encoder.transform(df_ca.item_id.values)\n",
    "# df_item_ca.item_id = item_encoder.transform(df_item_ca.item_id.values)\n",
    "\n",
    "\n",
    "# user_encoder = preprocessing.LabelEncoder().fit(df_ca.user_id.values)\n",
    "# df_ca.user_id = user_encoder.transform(df_ca.user_id.values)\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# item_encoder = preprocessing.LabelEncoder().fit(df_tn.item_id.values)\n",
    "# df_tn.item_id = item_encoder.transform(df_tn.item_id.values)\n",
    "# df_item_tn.item_id = item_encoder.transform(df_item_tn.item_id.values)\n",
    "\n",
    "\n",
    "# user_encoder = preprocessing.LabelEncoder().fit(df_tn.user_id.values)\n",
    "# df_tn.user_id = user_encoder.transform(df_tn.user_id.values)\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# item_encoder = preprocessing.LabelEncoder().fit(df_fl.item_id.values)\n",
    "# df_fl.item_id = item_encoder.transform(df_fl.item_id.values)\n",
    "# df_item_fl.item_id = item_encoder.transform(df_item_fl.item_id.values)\n",
    "\n",
    "\n",
    "# user_encoder = preprocessing.LabelEncoder().fit(df_fl.user_id.values)\n",
    "# df_fl.user_id = user_encoder.transform(df_fl.user_id.values)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# item_encoder = preprocessing.LabelEncoder().fit(df_pa.item_id.values)\n",
    "# df_pa.item_id = item_encoder.transform(df_pa.item_id.values)\n",
    "# df_item_pa.item_id = item_encoder.transform(df_item_pa.item_id.values)\n",
    "\n",
    "\n",
    "# user_encoder = preprocessing.LabelEncoder().fit(df_pa.user_id.values)\n",
    "# df_pa.user_id = user_encoder.transform(df_pa.user_id.values)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# item_encoder = preprocessing.LabelEncoder().fit(df_pd.item_id.values)\n",
    "# df_pd.item_id = item_encoder.transform(df_pd.item_id.values)\n",
    "# df_item_pd.item_id = item_encoder.transform(df_item_pd.item_id.values)\n",
    "\n",
    "\n",
    "# user_encoder = preprocessing.LabelEncoder().fit(df_pd.user_id.values)\n",
    "# df_pd.user_id = user_encoder.transform(df_pd.user_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"user_id\", \"item_id\", \"stars_x\"]].rename(columns={\"stars_x\": \"stars\"}).to_csv(\"../data/yelp/review.csv\", index=False)\n",
    "# df_ca[[\"user_id\", \"item_id\", \"stars_x\"]].rename(columns={\"stars_x\": \"stars\"}).to_csv(\"../data/yelp/review_ca.csv\", index=False)\n",
    "# df_tn[[\"user_id\", \"item_id\", \"stars_x\"]].rename(columns={\"stars_x\": \"stars\"}).to_csv(\"../data/yelp/review_tn.csv\", index=False)\n",
    "# df_fl[[\"user_id\", \"item_id\", \"stars_x\"]].rename(columns={\"stars_x\": \"stars\"}).to_csv(\"../data/yelp_fl/review_fl.csv\", index=False)\n",
    "# df_pa[[\"user_id\", \"item_id\", \"stars_x\"]].rename(columns={\"stars_x\": \"stars\"}).to_csv(\"../data/yelp_ca/review_pa.csv\", index=False)\n",
    "# df_pd[[\"user_id\", \"item_id\", \"stars_x\"]].rename(columns={\"stars_x\": \"stars\"}).to_csv(\"../data/yelp_pd/review_pd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item = df_item.rename(columns={\"name\": \"item_name\"})\n",
    "# df_item_ca = df_item_ca.rename(columns={\"name\": \"item_name\"})\n",
    "# df_item_tn = df_item_tn.rename(columns={\"name\": \"item_name\"})\n",
    "# df_item_fl = df_item_fl.rename(columns={\"name\": \"item_name\"})\n",
    "# df_item_pa = df_item_pa.rename(columns={\"name\": \"item_name\"})\n",
    "# df_item_pd = df_item_pd.rename(columns={\"name\": \"item_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item[[\"item_id\", \"item_name\"]].to_csv(\"../data/yelp/items.csv\", index=False)\n",
    "# df_item_ca[[\"item_id\", \"item_name\"]].to_csv(\"../data/yelp/items_ca.csv\", index=False)\n",
    "# df_item_tn[[\"item_id\", \"item_name\"]].to_csv(\"../data/yelp/items_tn.csv\", index=False)\n",
    "# df_item_fl[[\"item_id\", \"item_name\"]].to_csv(\"../data/yelp_fl/items_fl.csv\", index=False)\n",
    "# df_item_pa[[\"item_id\", \"item_name\"]].to_csv(\"../data/yelp_ca/items_pa.csv\", index=False)\n",
    "# df_item_pd[[\"item_id\", \"item_name\"]].to_csv(\"../data/yelp_pd/items_pd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_item = df_item[[\"item_id\", \"categories\"]].fillna(\"other\")\n",
    "# _df_item_ca = df_item_ca[[\"item_id\", \"categories\"]].fillna(\"other\")\n",
    "# _df_item_tn = df_item_tn[[\"item_id\", \"categories\"]].fillna(\"other\")\n",
    "# _df_item_fl = df_item_fl[[\"item_id\", \"categories\"]].fillna(\"other\")\n",
    "# _df_item_pa = df_item_pa[[\"item_id\", \"categories\"]].fillna(\"other\")\n",
    "# _df_item_pd = df_item_pd[[\"item_id\", \"categories\"]].fillna(\"other\")\n",
    "\n",
    "\n",
    "_df_item.categories  = _df_item.categories.str.split(\",\")\n",
    "# _df_item_ca.categories  = _df_item_ca.categories.str.split(\",\")\n",
    "# _df_item_tn.categories  = _df_item_tn.categories.str.split(\",\")\n",
    "# _df_item_fl.categories  = _df_item_fl.categories.str.split(\",\")\n",
    "# _df_item_pa.categories  = _df_item_pa.categories.str.split(\",\")\n",
    "# _df_item_pd.categories  = _df_item_pd.categories.str.split(\",\")\n",
    "\n",
    "\n",
    "_df_item = _df_item.rename(columns={\"categories\": \"metadata\"})\n",
    "# _df_item_ca = _df_item_ca.rename(columns={\"categories\": \"metadata\"})\n",
    "# _df_item_tn = _df_item_tn.rename(columns={\"categories\": \"metadata\"})\n",
    "# _df_item_fl = _df_item_fl.rename(columns={\"categories\": \"metadata\"})\n",
    "# _df_item_pa = _df_item_pa.rename(columns={\"categories\": \"metadata\"})\n",
    "# _df_item_pd = _df_item_pd.rename(columns={\"categories\": \"metadata\"})\n",
    "\n",
    "\n",
    "_df_item.to_csv(\"../data/yelp/items_metadata.csv\", index=False)\n",
    "# _df_item_ca.to_csv(\"../data/yelp/items_metadata_ca.csv\", index=False)\n",
    "# _df_item_tn.to_csv(\"../data/yelp/items_metadata_tn.csv\", index=False)\n",
    "# _df_item_fl.to_csv(\"../data/yelp_fl/items_metadata_fl.csv\", index=False)\n",
    "# _df_item_pa.to_csv(\"../data/yelp_ca/items_metadata_pa.csv\", index=False)\n",
    "# _df_item_pd.to_csv(\"../data/yelp_pd/items_metadata_pd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "from src.model.pmf import PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(preds, truth):\n",
    "    return np.sqrt(np.mean(np.square(preds-truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "epoches = 500\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "weight_decay = 0.1\n",
    "embedding_feature_size = 100\n",
    "ratio = 0.8\n",
    "lr = 0.0001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/yelp/review.csv\")\n",
    "df.head(), df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"user_id\", \"item_id\", \"stars\"]].values\n",
    "\n",
    "# Normalize rewards to [-1, 1]\n",
    "data[:,2] = 0.5*(data[:,2] - 3)\n",
    "\n",
    "# Shuffle data\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITEMS = df.item_id.max() + 1\n",
    "NUM_USERS = df.user_id.max() + 1\n",
    "\n",
    "print(NUM_USERS, NUM_ITEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = data[:int(ratio * data.shape[0])]\n",
    "vali_data = data[int(ratio * data.shape[0]): int((ratio+(1-ratio)/2)*data.shape[0])]\n",
    "test_data = data[int((ratio + (1 - ratio) / 2) * data.shape[0]) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuda=False\n",
    "\n",
    "# Get CUDA device if available\n",
    "cuda = torch.cuda.is_available()\n",
    " \n",
    "# Set device to CUDA or CPU, depending on availability and desire\n",
    "device = torch.device(\"cuda\" if cuda and no_cuda else \"cpu\")\n",
    " \n",
    "# Generate and apply seeds\n",
    "torch.manual_seed(seed=seed)\n",
    "if cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.manual_seed(seed=seed)\n",
    " \n",
    "# Specify number of workers for cuda\n",
    "kwargs = {\"num_workers\":1, \"pin_memory\":True} if cuda else {}\n",
    " \n",
    "# Construct Data Loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(torch.from_numpy(train_data), batch_size=batch_size, shuffle=False, **kwargs)\n",
    "test_data_loader = torch.utils.data.DataLoader(torch.from_numpy(test_data), batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PMF(n_users=NUM_USERS, n_items=NUM_ITEMS, n_factors=embedding_feature_size, no_cuda=no_cuda)\n",
    " \n",
    "# Move model to CUDA if CUDA selected\n",
    "if cuda and not no_cuda:\n",
    "    model.cuda()\n",
    "    print(\"Model moved to CUDA\")\n",
    " \n",
    "# Set loss function\n",
    "loss_function = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Set optimizer (uncomment Adam for adam)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training one epoch\n",
    "def train(epoch, train_data_loader):\n",
    "    # Initialize\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    " \n",
    "    # Go through batches\n",
    "    for batch_idx, ele in enumerate(train_data_loader):\n",
    "        # Zero optimizer gradient\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        # Extract user_id_nums: row 0, item_id_nums: col 1 , ratings: val 2\n",
    "        row = ele[:, 0]\n",
    "        col = ele[:, 1]\n",
    "        val = ele[:, 2]\n",
    " \n",
    "        # Set to variables\n",
    "        row = Variable(row.long())\n",
    "        if isinstance(col, list):\n",
    "            col = tuple(Variable(c.long()) for c in col)\n",
    "        else:\n",
    "            col = Variable(col.long())\n",
    "        val = Variable(val.float())\n",
    "\n",
    "        # Move data to CUDA\n",
    "        if cuda and not no_cuda:\n",
    "            row = row.cuda()\n",
    "            col = col.cuda()\n",
    "            val = val.cuda()\n",
    " \n",
    "        # Train\n",
    "        preds = model.forward(row, col)\n",
    "        loss = loss_function(preds, val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Update epoch loss\n",
    "        epoch_loss += loss.data\n",
    " \n",
    "    epoch_loss /= train_data_loader.dataset.shape[0]\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model\n",
    "count = 0\n",
    "train_loss_list = []\n",
    "last_vali_rmse = None\n",
    "train_rmse_list = []\n",
    "vali_rmse_list = []\n",
    "print(\"parameters are: train ratio:{:f},batch_size:{:d}, epoches:{:d}, weight_decay:{:f}\".format(ratio, batch_size, epoches, weight_decay))\n",
    "print(model)\n",
    "\n",
    "# Go through epochs\n",
    "for epoch in range(1, epoches+1):\n",
    "\n",
    "    # Train epoch\n",
    "    train_epoch_loss = train(epoch, train_data_loader)\n",
    "\n",
    "    # Get epoch loss\n",
    "    train_loss_list.append(train_epoch_loss.cpu())\n",
    "\n",
    "    # Move validation data to CUDA\n",
    "    if cuda and not no_cuda:\n",
    "        vali_row = Variable(torch.from_numpy(vali_data[:, 0]).long()).cuda()\n",
    "        vali_col = Variable(torch.from_numpy(vali_data[:, 1]).long()).cuda()\n",
    "    else:\n",
    "        vali_row = Variable(torch.from_numpy(vali_data[:, 0]).long())\n",
    "        vali_col = Variable(torch.from_numpy(vali_data[:, 1]).long())\n",
    "\n",
    "    # Get validation predictions\n",
    "    vali_preds = model.predict(vali_row, vali_col)\n",
    "\n",
    "    # Calculate train rmse loss\n",
    "    train_rmse = np.sqrt(train_epoch_loss.cpu())\n",
    "\n",
    "    # Calculate validation rmse loss\n",
    "    if cuda and not no_cuda:\n",
    "        vali_rmse = RMSE(vali_preds.cpu().data.numpy(), vali_data[:, 2])\n",
    "    else:\n",
    "        vali_rmse = RMSE(vali_preds.data.numpy(), vali_data[:, 2])\n",
    "\n",
    "    # Add losses to rmse loss lists\n",
    "    train_rmse_list.append(train_rmse)\n",
    "    vali_rmse_list.append(vali_rmse)\n",
    "\n",
    "    print(\"Training epoch:{: d}, training rmse:{: .6f}, vali rmse:{:.6f}\". \\\n",
    "            format(epoch, train_rmse, vali_rmse))\n",
    "\n",
    "    # Early stop condition\n",
    "    if last_vali_rmse and last_vali_rmse < vali_rmse:\n",
    "        break\n",
    "    else:\n",
    "        last_vali_rmse = vali_rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Model\n",
    "\n",
    "# Move test set to CUDA\n",
    "if cuda:\n",
    "    test_row = Variable(torch.from_numpy(test_data[:, 0]).long()).cuda()\n",
    "    test_col = Variable(torch.from_numpy(test_data[:, 1]).long()).cuda()\n",
    "else:\n",
    "    test_row = Variable(torch.from_numpy(test_data[:, 0]).long())\n",
    "    test_col = Variable(torch.from_numpy(test_data[:, 1]).long())\n",
    " \n",
    "# Get test predictions\n",
    "preds = model.predict(test_row, test_col)\n",
    " \n",
    "# Get test rmse loss\n",
    "if cuda:\n",
    "    test_rmse = RMSE(preds.cpu().data.numpy(), test_data[:, 2])\n",
    "else:\n",
    "    test_rmse = RMSE(preds.data.numpy(), test_data[:, 2])\n",
    "print(\"Test rmse: {:f}\".format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots\n",
    "plt.figure(1)\n",
    "plt.plot(range(1, len(train_rmse_list)+1), train_rmse_list, color=\"r\", label=\"train rmse\")\n",
    "plt.plot(range(1, len(vali_rmse_list)+1), vali_rmse_list, color=\"b\", label=\"test rmse\")\n",
    "plt.legend()\n",
    "plt.annotate(r\"train=%f\" % (train_rmse_list[-1]), xy=(len(train_rmse_list), train_rmse_list[-1]),\n",
    "             xycoords=\"data\", xytext=(-30, 30), textcoords=\"offset points\", fontsize=10,\n",
    "             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=.2\"))\n",
    "plt.annotate(r\"vali=%f\" % (vali_rmse_list[-1]), xy=(len(vali_rmse_list), vali_rmse_list[-1]),\n",
    "             xycoords=\"data\", xytext=(-30, 30), textcoords=\"offset points\", fontsize=10,\n",
    "             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=.2\"))\n",
    "plt.xlim([1, len(train_rmse_list)+10])\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE Curve in Training Process\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "path_to_trained_pmf = \"../model/pmf/yelp_pd_emb_{:d}_ratio_{:f}_bs_{:d}_e_{:d}_wd_{:f}_lr_{:f}_trained_pmf.pt\".format(embedding_feature_size, ratio, batch_size, len(train_rmse_list), weight_decay, lr)\n",
    "torch.save(model.state_dict(), path_to_trained_pmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =  -2\n",
    "(model.predict(\n",
    "    torch.tensor([data[idx][0]]).long().to(\"cuda\"), \n",
    "    torch.tensor([data[idx][1]]).long().to(\"cuda\")\n",
    ").cpu().data[0] + 1) / 2, (data[idx][2] + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_dict = {user: [] for user in set(df[\"user_id\"])}\n",
    "\n",
    "ratings_df_gen = df.iterrows()\n",
    "users_dict_positive_items = {\n",
    "    user: [] for user in set(df[\"user_id\"])\n",
    "}\n",
    "for data in ratings_df_gen:\n",
    "    users_dict[data[1][\"user_id\"]].append(\n",
    "        (data[1][\"item_id\"], data[1][\"stars\"])\n",
    "    )\n",
    "    if data[1][\"stars\"] >= 4:\n",
    "        users_dict_positive_items[data[1][\"user_id\"]].append(\n",
    "            (data[1][\"item_id\"], data[1][\"stars\"])\n",
    "        )\n",
    "users_history_lens = [\n",
    "    len(users_dict_positive_items[u])\n",
    "    for u in set(df[\"user_id\"])\n",
    "]\n",
    "\n",
    "users_num = max(df[\"user_id\"]) + 1\n",
    "items_num = max(df[\"item_id\"]) + 1\n",
    "\n",
    "print(users_num, items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_num = int(users_num * 0.8)\n",
    "train_users_dict = {k: users_dict.get(k) for k in range(0, train_users_num - 1)}\n",
    "train_users_history_lens = users_history_lens[:train_users_num]\n",
    "\n",
    "# Evaluating setting\n",
    "eval_users_num = int(users_num * 0.2)\n",
    "eval_users_dict = {\n",
    "    k: users_dict[k] for k in range(users_num - eval_users_num, users_num)\n",
    "}\n",
    "eval_users_history_lens = users_history_lens[-eval_users_num:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "with open(\"../data/yelp_pd/train_users_dict.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_users_dict, file)\n",
    "\n",
    "with open(\"../data/yelp_pd/train_users_history_lens.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_users_history_lens, file)\n",
    "\n",
    "with open(\"../data/yelp_pd/eval_users_dict.pkl\", \"wb\") as file:\n",
    "    pickle.dump(eval_users_dict, file)\n",
    "\n",
    "with open(\"../data/yelp_pd/eval_users_history_lens.pkl\", \"wb\") as file:\n",
    "    pickle.dump(eval_users_history_lens, file)\n",
    "\n",
    "with open(\"../data/yelp_pd/users_history_lens.pkl\", \"wb\") as file:\n",
    "    pickle.dump(users_history_lens, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.geometric(p=0.35, size=items_num)\n",
    "w = z%10 \n",
    "w = [i if i > 0 else 10 for i in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_group = {i: w[i] for i in range(items_num)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/yelp_pd/item_groups.pkl\", \"wb\") as file:\n",
    "    pickle.dump(item_group, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23b1c047fd19bf20d4111f807c9e6bee9fdce0e396893bab828ebbcdf9a68cad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
