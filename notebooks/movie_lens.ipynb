{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "from src.model.pmf import PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(preds, truth):\n",
    "    return np.sqrt(np.mean(np.square(preds-truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "epoches = 1000\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "weight_decay = 0.1\n",
    "embedding_feature_size = 100\n",
    "ratio = 0.8\n",
    "lr = 0.0001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/ml-100k/ratings.csv\")\n",
    "df[\"rating\"] = df[\"rating\"].astype(\"float\")\n",
    "df = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"user_id\", \"item_id\", \"rating\"]].values\n",
    " \n",
    "# Normalize rewards to [-1, 1]\n",
    "data[:,2] = 0.5*(data[:,2] - 3)\n",
    "\n",
    "# Shuffle data\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = data[:int(ratio * data.shape[0])]\n",
    "vali_data = data[int(ratio * data.shape[0]): int((ratio+(1-ratio)/2)*data.shape[0])]\n",
    "test_data = data[int((ratio + (1 - ratio) / 2) * data.shape[0]) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITEMS = df.item_id.max() + 1\n",
    "NUM_USERS = df.user_id.max() + 1\n",
    "\n",
    "print(NUM_USERS, NUM_ITEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CUDA device if available\n",
    "cuda = torch.cuda.is_available()\n",
    " \n",
    "# Set device to CUDA or CPU, depending on availability and desire\n",
    "device = torch.device(\"cuda\" if cuda and no_cuda else \"cpu\")\n",
    " \n",
    "# Generate and apply seeds\n",
    "torch.manual_seed(seed=seed)\n",
    "if cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.manual_seed(seed=seed)\n",
    " \n",
    "# Specify number of workers for cuda\n",
    "kwargs = {\"num_workers\":1, \"pin_memory\":True} if cuda else {}\n",
    " \n",
    "# Construct Data Loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(torch.from_numpy(train_data), batch_size=batch_size, shuffle=False, **kwargs)\n",
    "test_data_loader = torch.utils.data.DataLoader(torch.from_numpy(test_data), batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PMF(n_users=NUM_USERS, n_items=NUM_ITEMS, n_factors=embedding_feature_size, no_cuda=no_cuda)\n",
    " \n",
    "# Move model to CUDA if CUDA selected\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    print(\"Model moved to CUDA\")\n",
    " \n",
    "# Set loss function\n",
    "loss_function = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Set optimizer (uncomment Adam for adam)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training one epoch\n",
    "def train(epoch, train_data_loader):\n",
    "    # Initialize\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    " \n",
    "    # Go through batches\n",
    "    for batch_idx, ele in enumerate(train_data_loader):\n",
    "        # Zero optimizer gradient\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        # Extract user_id_nums: row 0, item_id_nums: col 1 , ratings: val 2\n",
    "        row = ele[:, 0]\n",
    "        col = ele[:, 1]\n",
    "        val = ele[:, 2]\n",
    " \n",
    "        # Set to variables\n",
    "        row = Variable(row.long())\n",
    "        if isinstance(col, list):\n",
    "            col = tuple(Variable(c.long()) for c in col)\n",
    "        else:\n",
    "            col = Variable(col.long())\n",
    "        val = Variable(val.float())\n",
    "\n",
    "        # Move data to CUDA\n",
    "        if cuda:\n",
    "            row = row.cuda()\n",
    "            col = col.cuda()\n",
    "            val = val.cuda()\n",
    " \n",
    "        # Train\n",
    "        preds = model.forward(row, col)\n",
    "        loss = loss_function(preds, val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Update epoch loss\n",
    "        epoch_loss += loss.data\n",
    " \n",
    "    epoch_loss /= train_data_loader.dataset.shape[0]\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model\n",
    "\n",
    "train_loss_list = []\n",
    "last_vali_rmse = None\n",
    "train_rmse_list = []\n",
    "vali_rmse_list = []\n",
    "print(\"parameters are: train ratio:{:f},batch_size:{:d}, epoches:{:d}, weight_decay:{:f}\".format(ratio, batch_size, epoches, weight_decay))\n",
    "print(model)\n",
    "\n",
    "# Go through epochs\n",
    "for epoch in range(1, epoches+1):\n",
    " \n",
    "    # Train epoch\n",
    "    train_epoch_loss = train(epoch, train_data_loader)\n",
    " \n",
    "    # Get epoch loss\n",
    "    train_loss_list.append(train_epoch_loss.cpu())\n",
    " \n",
    "    # Move validation data to CUDA\n",
    "    if cuda:\n",
    "        vali_row = Variable(torch.from_numpy(vali_data[:, 0]).long()).cuda()\n",
    "        vali_col = Variable(torch.from_numpy(vali_data[:, 1]).long()).cuda()\n",
    "    else:\n",
    "        vali_row = Variable(torch.from_numpy(vali_data[:, 0]).long())\n",
    "        vali_col = Variable(torch.from_numpy(vali_data[:, 1]).long())\n",
    " \n",
    "    # Get validation predictions\n",
    "    vali_preds = model.predict(vali_row, vali_col)\n",
    " \n",
    "    # Calculate train rmse loss\n",
    "    train_rmse = np.sqrt(train_epoch_loss.cpu())\n",
    " \n",
    "    # Calculate validation rmse loss\n",
    "    if cuda:\n",
    "        vali_rmse = RMSE(vali_preds.cpu().data.numpy(), vali_data[:, 2])\n",
    "    else:\n",
    "        vali_rmse = RMSE(vali_preds.data.numpy(), vali_data[:, 2])\n",
    " \n",
    "    # Add losses to rmse loss lists\n",
    "    train_rmse_list.append(train_rmse)\n",
    "    vali_rmse_list.append(vali_rmse)\n",
    " \n",
    "    print(\"Training epoch:{: d}, training rmse:{: .6f}, vali rmse:{:.6f}\". \\\n",
    "              format(epoch, train_rmse, vali_rmse))\n",
    " \n",
    "    # Early stop condition\n",
    "    if last_vali_rmse and last_vali_rmse < vali_rmse:\n",
    "        break\n",
    "    else:\n",
    "      last_vali_rmse = vali_rmse\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Model\n",
    "\n",
    "# Move test set to CUDA\n",
    "if cuda:\n",
    "    test_row = Variable(torch.from_numpy(test_data[:, 0]).long()).cuda()\n",
    "    test_col = Variable(torch.from_numpy(test_data[:, 1]).long()).cuda()\n",
    "else:\n",
    "    test_row = Variable(torch.from_numpy(test_data[:, 0]).long())\n",
    "    test_col = Variable(torch.from_numpy(test_data[:, 1]).long())\n",
    " \n",
    "# Get test predictions\n",
    "preds = model.predict(test_row, test_col)\n",
    " \n",
    "# Get test rmse loss\n",
    "if cuda:\n",
    "    test_rmse = RMSE(preds.cpu().data.numpy(), test_data[:, 2])\n",
    "else:\n",
    "    test_rmse = RMSE(preds.data.numpy(), test_data[:, 2])\n",
    "print(\"Test rmse: {:f}\".format(test_rmse))\n",
    " \n",
    "# Create plots\n",
    "plt.figure(1)\n",
    "plt.figure(1).patch.set_facecolor(\"white\")\n",
    "plt.plot(range(1, len(train_rmse_list)+1), train_rmse_list, color=\"r\", label=\"train rmse\")\n",
    "plt.plot(range(1, len(vali_rmse_list)+1), vali_rmse_list, color=\"b\", label=\"test rmse\")\n",
    "plt.legend()\n",
    "plt.annotate(r\"train=%f\" % (train_rmse_list[-1]), xy=(len(train_rmse_list), train_rmse_list[-1]),\n",
    "             xycoords=\"data\", xytext=(-30, 30), textcoords=\"offset points\", fontsize=10,\n",
    "             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=.2\"))\n",
    "plt.annotate(r\"vali=%f\" % (vali_rmse_list[-1]), xy=(len(vali_rmse_list), vali_rmse_list[-1]),\n",
    "             xycoords=\"data\", xytext=(-30, 30), textcoords=\"offset points\", fontsize=10,\n",
    "             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=.2\"))\n",
    "plt.xlim([1, len(train_rmse_list)+10])\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE Curve in Training Process\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "path_to_trained_pmf = \"../model/pmf/ml_100k_emb_{:d}_ratio_{:f}_bs_{:d}_e_{:d}_wd_{:f}_lr_{:f}_trained_pmf.pt\".format(embedding_feature_size, ratio, batch_size, len(train_rmse_list), weight_decay, lr)\n",
    "torch.save(model.state_dict(), path_to_trained_pmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =  0 \n",
    "model.predict(\n",
    "    torch.tensor([data[idx][0]]).long().to(\"cuda\"), \n",
    "    torch.tensor([data[idx][1]]).long().to(\"cuda\")\n",
    ").cpu().data[0] * 2 + 3, data[idx][2] * 2 + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "from src.model.pmf import PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = \"movie_lens_100k\"\n",
    "dataset_path = \"../data/{}_output_path.json\".format(data_version)\n",
    "\n",
    "with open(dataset_path) as json_file:\n",
    "    _dataset_path = json.load(json_file)\n",
    "\n",
    "dataset = {}\n",
    "with open(os.path.join(\"..\", _dataset_path[\"train_users_dict\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"train_users_dict\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"train_users_history_lens\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"train_users_history_lens\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"eval_users_dict\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"eval_users_dict\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"eval_users_history_lens\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"eval_users_history_lens\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"users_history_lens\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"users_history_lens\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"item_groups\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"item_groups\"] = pickle.load(pkl_file)\n",
    "\n",
    "items_df = pd.read_csv(os.path.join(\"..\", _dataset_path[\"items_df\"]))\n",
    "items_metadata_df = pd.read_csv(os.path.join(\"..\", _dataset_path[\"items_metadata\"]))\n",
    "users_df = pd.read_csv(os.path.join(\"..\",_dataset_path[\"users_df\"]))\n",
    "ratings_df = pd.read_csv(os.path.join(\"..\",_dataset_path[\"ratings_df\"]))\n",
    "\n",
    "\n",
    "items_df = items_df.set_index(\"item_id\")\n",
    "items_metadata_df = items_metadata_df.set_index(\"item_id\")\n",
    "users_df = users_df.set_index(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bg(bg_alpha=.01, figsize=(13, 9), emb_2d=None):\n",
    "    \"\"\"Create and return a plot of all our movie embeddings with very low opacity.\n",
    "    (Intended to be used as a basis for further - more prominent - plotting of a \n",
    "    subset of movies. Having the overall shape of the map space in the background is\n",
    "    useful for context.)\n",
    "    \"\"\"\n",
    "    if emb_2d is None:\n",
    "        emb_2d = embs\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    X = emb_2d[:, 0]\n",
    "    Y = emb_2d[:, 1]\n",
    "    ax.scatter(X, Y, alpha=bg_alpha)\n",
    "    return ax\n",
    "    \n",
    "def plot_with_annotations(label_indices, text=True, labels=None, alpha=1, **kwargs):\n",
    "    ax = plot_bg(**kwargs)\n",
    "    Xlabeled = embs[label_indices, 0]\n",
    "    Ylabeled = embs[label_indices, 1]\n",
    "    if labels is not None:\n",
    "        for x, y, label in zip(Xlabeled, Ylabeled, labels):\n",
    "            ax.scatter(x, y, alpha=alpha, label=label, marker='1',\n",
    "                       s=90,\n",
    "                      )\n",
    "        fig.legend()\n",
    "    else:\n",
    "        ax.scatter(Xlabeled, Ylabeled, alpha=alpha, color='green')\n",
    "    \n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_list_similarity(predicted, feature_df):\n",
    "    recs_content = feature_df.loc[predicted]\n",
    "    recs_content = recs_content\n",
    "    similarity = cosine_similarity(X=recs_content, dense_output=False)\n",
    "\n",
    "    # #get indicies for upper right triangle w/o diagonal\n",
    "    upper_right = np.triu_indices(similarity.shape[0], k=1)\n",
    "    upper_right\n",
    "\n",
    "    # #calculate average similarity score of all recommended items in list\n",
    "    ils_single_user = np.mean(similarity[upper_right])\n",
    "    return ils_single_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMF Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_version == \"movie_lens_100k\":\n",
    "\n",
    "    reward_model = PMF(943, 1682, 50).to(\"cuda\")\n",
    "    reward_model.load_state_dict(\n",
    "        torch.load(\n",
    "            \"../model/pmf/emb_50_ratio_0.800000_bs_1000_e_258_wd_0.100000_lr_0.000100_trained_pmf.pt\",\n",
    "            map_location=torch.device(\"cuda\"),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    reward_model = PMF(6040, 3883, 100).to(\"cuda\")\n",
    "    reward_model.load_state_dict(\n",
    "        torch.load(\n",
    "            \"../model/pmf/ml_1m_emb_100_ratio_0.800000_bs_1000_e_457_wd_0.100000_lr_0.000100_trained_pmf.pt\",\n",
    "            map_location=torch.device(\"cuda\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "user_embeddings = reward_model.user_embeddings.weight.data\n",
    "item_embeddings = reward_model.item_embeddings.weight.data\n",
    "\n",
    "users_pmf_emb = pd.DataFrame(user_embeddings[users_df.index.values].cpu().numpy().tolist())\n",
    "item_pmf_emb = pd.DataFrame(item_embeddings[items_df.index.values].cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(users_pmf_emb.values)\n",
    "# Add to dataframe for convenience\n",
    "users_pmf_emb['x'] = embs[:, 0]\n",
    "users_pmf_emb['y'] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(users_pmf_emb.x, users_pmf_emb.y, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(item_pmf_emb.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df['x'] = embs[:, 0]\n",
    "items_df['y'] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(items_df[items_df[\"title\"].str.startswith('Star Trek')].index, text=False, alpha=.4, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(items_df[items_df[\"title\"].str.startswith('Star Trek')].index.values, item_pmf_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings_df[ratings_df[\"rating\"] > 3].groupby(\"user_id\").agg({\"item_id\": lambda x: x.tolist()}).reset_index()\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(lambda x: single_list_similarity(x, item_pmf_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(users[\"user_id\"].values, users_pmf_emb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(users, x=\"user_id\", y=\"similarity\", title=\"Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer(\"all-MiniLM-L12-v1\") # bert-base-nli-mean-tokens / all-MiniLM-L6-v2\n",
    "sentence_embeddings = bert.encode(items_df[\"item_name\"].tolist())\n",
    "sentence_embeddings = pd.DataFrame(sentence_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(sentence_embeddings.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df['x'] = embs[:, 0]\n",
    "items_df['y'] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(items_df[items_df[\"title\"].str.startswith('Star Trek')].index, text=False, alpha=.4, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(items_df[items_df[\"title\"].str.startswith('Star Trek')].index.values, sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings_df[ratings_df[\"rating\"] > 3].groupby(\"user_id\").agg({\"item_id\": lambda x: x.tolist()}).reset_index()\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(lambda x: single_list_similarity(x, sentence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(users, x=\"user_id\", y=\"similarity\", title=\"Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_intent = items_metadata_df[\n",
    "    items_metadata_df.index.isin([0, 1, 2, 3])\n",
    "]\n",
    "user_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_embeddings = items_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(genre_embeddings.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df['x'] = embs[:, 0]\n",
    "items_df['y'] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(items_df[items_df[\"title\"].str.startswith('Star Trek')].index, text=False, alpha=.4, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(items_df[items_df[\"title\"].str.startswith('Star Trek')].index.values, genre_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings_df[ratings_df[\"rating\"] > 3].groupby(\"user_id\").agg({\"item_id\": lambda x: x.tolist()}).reset_index()\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(lambda x: single_list_similarity(x, genre_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(users, x=\"user_id\", y=\"similarity\", title=\"Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title + Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer(\"all-MiniLM-L6-v2\") # bert-base-nli-mean-tokens / all-MiniLM-L6-v2\n",
    "sentence_embeddings = bert.encode(items_df[\"title\"].tolist())\n",
    "sentence_embeddings = pd.DataFrame(sentence_embeddings.tolist())\n",
    "\n",
    "genre_embeddings = items_metadata_df\n",
    "\n",
    "join_embeddings = pd.concat([genre_embeddings, sentence_embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(join_embeddings.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df['x'] = embs[:, 0]\n",
    "items_df['y'] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(items_df[items_df[\"title\"].str.startswith('Star Trek')].index, text=False, alpha=.4, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(items_df[items_df[\"title\"].str.startswith('Star Trek')].index.values, join_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings_df[ratings_df[\"rating\"] > 3].groupby(\"user_id\").agg({\"item_id\": lambda x: x.tolist()}).reset_index()\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(lambda x: single_list_similarity(x, join_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(users, x=\"user_id\", y=\"similarity\", title=\"Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = [col for col in items_metadata_df]\n",
    "items_metadata_df[\"genres\"] = items_metadata_df[filter_col].dot(pd.Index(filter_col) + ', ' ).str.strip(', ')\n",
    "df = pd.concat([items_df, items_metadata_df], axis=1)[[\"title\", \"release_date\", \"genres\"]]\n",
    "df[\"input\"] = df.apply(lambda x: f\"{x['release_date']}, {x['genres']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
    "sentence_embeddings = bert.encode(df[\"genres\"].tolist())\n",
    "sentence_embeddings = pd.DataFrame(sentence_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(sentence_embeddings.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df['x'] = embs[:, 0]\n",
    "items_df['y'] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(items_df[items_df[\"title\"].str.startswith('Star Trek')].index, text=False, alpha=.4, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(items_df[items_df[\"title\"].str.startswith('Star Trek')].index.values, sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings_df[ratings_df[\"rating\"] > 3].groupby(\"user_id\").agg({\"item_id\": lambda x: x.tolist()}).reset_index()\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(lambda x: single_list_similarity(x, sentence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(users, x=\"user_id\", y=\"similarity\", title=\"Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity.describe()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23b1c047fd19bf20d4111f807c9e6bee9fdce0e396893bab828ebbcdf9a68cad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rsrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
