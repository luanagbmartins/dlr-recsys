{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.model.pmf import PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(preds, truth):\n",
    "    return np.sqrt(np.mean(np.square(preds - truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "epoches = 1000\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "weight_decay = 0.1\n",
    "embedding_feature_size = 100\n",
    "ratio = 0.8\n",
    "lr = 0.0001\n",
    "momentum = 0.9\n",
    "k_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/ml-100k/ratings.csv\")\n",
    "df[\"rating\"] = df[\"rating\"].astype(\"float\")\n",
    "df = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "def split_train_test(data, train_ratio=0.8):\n",
    "    # Lista para armazenar os subsets de treino e teste\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    for _, group in data.groupby(\"user_id\"):\n",
    "        # Ordena as interações por timestamp\n",
    "        group = group.sort_values(\"timestamp\")\n",
    "\n",
    "        # Calcula o ponto de corte para o treino (80% das interações)\n",
    "        split_point = ceil(len(group) * train_ratio)\n",
    "\n",
    "        # Separa o conjunto de treino e teste\n",
    "        train_list.append(group.iloc[:split_point])\n",
    "        test_list.append(group.iloc[split_point:])\n",
    "\n",
    "    # Concatena todos os subsets de treino e teste\n",
    "    train_data = pd.concat(train_list)\n",
    "    test_data = pd.concat(test_list)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# Exemplo de uso:\n",
    "# df é o seu DataFrame com as colunas ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "train_df, test_df = split_train_test(df)\n",
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "# # Normalize rewards to [-1, 1]\n",
    "train_data = train_df[[\"user_id\", \"item_id\", \"rating\"]].values\n",
    "# train_data[:, 2] = 0.5 * (train_data[:, 2] - 3)\n",
    "\n",
    "test_data = test_df[[\"user_id\", \"item_id\", \"rating\"]].values\n",
    "# test_data[:, 2] = 0.5 * (test_data[:, 2] - 3)\n",
    "\n",
    "# # Shuffle data\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = df[[\"user_id\", \"item_id\", \"rating\"]].values\n",
    "# np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITEMS = df.item_id.max() + 1\n",
    "NUM_USERS = df.user_id.max() + 1\n",
    "\n",
    "print(NUM_USERS, NUM_ITEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CUDA device if available\n",
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)\n",
    "\n",
    "# Set device to CUDA or CPU, depending on availability and desire\n",
    "device = torch.device(\"cuda\" if cuda and no_cuda else \"cpu\")\n",
    "\n",
    "# Generate and apply seeds\n",
    "torch.manual_seed(seed=seed)\n",
    "if cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.manual_seed(seed=seed)\n",
    "\n",
    "# Specify number of workers for cuda\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PMF(\n",
    "    n_users=NUM_USERS,\n",
    "    n_items=NUM_ITEMS,\n",
    "    n_factors=embedding_feature_size,\n",
    "    no_cuda=no_cuda,\n",
    ")\n",
    "\n",
    "# Move model to CUDA if CUDA selected\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    print(\"Model moved to CUDA\")\n",
    "\n",
    "# Set loss function\n",
    "loss_function = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Set optimizer (uncomment Adam for adam)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum\n",
    ")\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training one epoch\n",
    "def train(epoch, train_data_loader):\n",
    "    # Initialize\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Go through batches\n",
    "    for batch_idx, ele in enumerate(train_data_loader):\n",
    "        # Zero optimizer gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract user_id_nums: row 0, item_id_nums: col 1 , ratings: val 2\n",
    "        row = ele[:, 0]\n",
    "        col = ele[:, 1]\n",
    "        val = ele[:, 2]\n",
    "\n",
    "        # Set to variables\n",
    "        row = Variable(row.long())\n",
    "        if isinstance(col, list):\n",
    "            col = tuple(Variable(c.long()) for c in col)\n",
    "        else:\n",
    "            col = Variable(col.long())\n",
    "        val = Variable(val.float())\n",
    "\n",
    "        # Move data to CUDA\n",
    "        if cuda:\n",
    "            row = row.cuda()\n",
    "            col = col.cuda()\n",
    "            val = val.cuda()\n",
    "\n",
    "        # Train\n",
    "        preds = model.forward(row, col)\n",
    "        loss = loss_function(preds, val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update epoch loss\n",
    "        epoch_loss += loss.data\n",
    "\n",
    "    epoch_loss /= train_data_loader.dataset.shape[0]\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.epochs_without_improvement = 0\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if (\n",
    "            self.best_score is None\n",
    "            or (current_score - self.best_score) < self.min_delta\n",
    "        ):\n",
    "            self.best_score = current_score\n",
    "            self.epochs_without_improvement = 0\n",
    "        else:\n",
    "            self.epochs_without_improvement += 1\n",
    "\n",
    "        if self.epochs_without_improvement >= self.patience:\n",
    "            return True  # Indica que o treinamento deve ser interrompido\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model\n",
    "train_loss_list = []\n",
    "train_rmse_list = []\n",
    "vali_rmse_list = []\n",
    "print(\n",
    "    \"parameters are: train ratio:{:f},batch_size:{:d}, epoches:{:d}, weight_decay:{:f}\".format(\n",
    "        ratio, batch_size, epoches, weight_decay\n",
    "    )\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "# for fold, (train_idx, test_idx) in enumerate(kf.split(train_data)):\n",
    "#     print(f\"Fold {fold + 1}\")\n",
    "print(\"-------\")\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "# Define the data loaders for the current fold\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=batch_size,\n",
    "    # sampler=torch.utils.data.SubsetRandomSampler(train_idx),\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=batch_size,\n",
    "    # sampler=torch.utils.data.SubsetRandomSampler(test_idx),\n",
    ")\n",
    "\n",
    "# Go through epochs\n",
    "for epoch in range(1, epoches + 1):\n",
    "    # Train epoch\n",
    "    train_epoch_loss = train(epoch, train_loader)\n",
    "\n",
    "    # Get epoch loss\n",
    "    train_loss_list.append(train_epoch_loss.cpu())\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(\"cuda\")\n",
    "            vali_preds = model.predict(\n",
    "                data[:, 0].long().cuda(), data[:, 1].long().cuda()\n",
    "            )\n",
    "            vali_rmse = RMSE(\n",
    "                vali_preds.cpu().data.numpy(), data[:, 2].cpu().data.numpy()\n",
    "            )\n",
    "\n",
    "    train_rmse = np.sqrt(train_epoch_loss.cpu())\n",
    "    train_rmse_list.append(train_rmse)\n",
    "    vali_rmse_list.append(vali_rmse)\n",
    "\n",
    "    print(\n",
    "        \"Training epoch:{: d}, training rmse:{: .6f}, vali rmse:{:.6f}\".format(\n",
    "            epoch, train_rmse, vali_rmse\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Early stop condition\n",
    "    if early_stopping(vali_rmse):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Model\n",
    "# Move test set to CUDA\n",
    "if cuda:\n",
    "    test_row = Variable(torch.from_numpy(test_data[:, 0]).long()).cuda()\n",
    "    test_col = Variable(torch.from_numpy(test_data[:, 1]).long()).cuda()\n",
    "else:\n",
    "    test_row = Variable(torch.from_numpy(test_data[:, 0]).long())\n",
    "    test_col = Variable(torch.from_numpy(test_data[:, 1]).long())\n",
    "\n",
    "# Get test predictions\n",
    "preds = model.predict(test_row, test_col)\n",
    "\n",
    "# Get test rmse loss\n",
    "if cuda:\n",
    "    test_rmse = RMSE(preds.cpu().data.numpy(), test_data[:, 2])\n",
    "else:\n",
    "    test_rmse = RMSE(preds.data.numpy(), test_data[:, 2])\n",
    "print(\"Test rmse: {:f}\".format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "path_to_trained_pmf = \"../model/pmf/ml_100k_emb_{:d}_ratio_{:f}_bs_{:d}_e_{:d}_wd_{:f}_lr_{:f}_trained_pmf.pt\".format(\n",
    "    embedding_feature_size, ratio, batch_size, len(train_rmse_list), weight_decay, lr\n",
    ")\n",
    "torch.save(model.state_dict(), path_to_trained_pmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "model.predict(\n",
    "    torch.tensor([data[idx][0]]).long().to(\"cuda\"),\n",
    "    torch.tensor([data[idx][1]]).long().to(\"cuda\"),\n",
    ").cpu().data[0] * 2 + 3, data[idx][2] * 2 + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.model.pmf import PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = \"movie_lens_100k\"\n",
    "dataset_path = \"../data/{}_output_path.json\".format(data_version)\n",
    "\n",
    "with open(dataset_path) as json_file:\n",
    "    _dataset_path = json.load(json_file)\n",
    "\n",
    "dataset = {}\n",
    "with open(os.path.join(\"..\", _dataset_path[\"train_users_dict\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"train_users_dict\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(\n",
    "    os.path.join(\"..\", _dataset_path[\"train_users_history_lens\"]), \"rb\"\n",
    ") as pkl_file:\n",
    "    dataset[\"train_users_history_lens\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"eval_users_dict\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"eval_users_dict\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(\n",
    "    os.path.join(\"..\", _dataset_path[\"eval_users_history_lens\"]), \"rb\"\n",
    ") as pkl_file:\n",
    "    dataset[\"eval_users_history_lens\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"users_history_lens\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"users_history_lens\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"item_groups\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"item_groups\"] = pickle.load(pkl_file)\n",
    "\n",
    "items_df = pd.read_csv(os.path.join(\"..\", _dataset_path[\"items_df\"]))\n",
    "items_metadata_df = pd.read_csv(os.path.join(\"..\", _dataset_path[\"items_metadata\"]))\n",
    "users_df = pd.read_csv(os.path.join(\"..\", _dataset_path[\"users_df\"]))\n",
    "ratings_df = pd.read_csv(os.path.join(\"..\", _dataset_path[\"ratings_df\"]))\n",
    "\n",
    "\n",
    "items_df = items_df.set_index(\"item_id\")\n",
    "items_metadata_df = items_metadata_df.set_index(\"item_id\")\n",
    "users_df = users_df.set_index(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(ratings_df.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bg(bg_alpha=0.01, figsize=(13, 9), emb_2d=None):\n",
    "    \"\"\"Create and return a plot of all our movie embeddings with very low opacity.\n",
    "    (Intended to be used as a basis for further - more prominent - plotting of a\n",
    "    subset of movies. Having the overall shape of the map space in the background is\n",
    "    useful for context.)\n",
    "    \"\"\"\n",
    "    if emb_2d is None:\n",
    "        emb_2d = embs\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    X = emb_2d[:, 0]\n",
    "    Y = emb_2d[:, 1]\n",
    "    ax.scatter(X, Y, alpha=bg_alpha)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_with_annotations(label_indices, text=True, labels=None, alpha=1, **kwargs):\n",
    "    ax = plot_bg(**kwargs)\n",
    "    Xlabeled = embs[label_indices, 0]\n",
    "    Ylabeled = embs[label_indices, 1]\n",
    "    if labels is not None:\n",
    "        for x, y, label in zip(Xlabeled, Ylabeled, labels):\n",
    "            ax.scatter(\n",
    "                x,\n",
    "                y,\n",
    "                alpha=alpha,\n",
    "                label=label,\n",
    "                marker=\"1\",\n",
    "                s=90,\n",
    "            )\n",
    "        fig.legend()\n",
    "    else:\n",
    "        ax.scatter(Xlabeled, Ylabeled, alpha=alpha, color=\"green\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_list_similarity(predicted, feature_df):\n",
    "    recs_content = feature_df.loc[predicted]\n",
    "    recs_content = recs_content\n",
    "    similarity = cosine_similarity(X=recs_content, dense_output=False)[\n",
    "        np.triu_indices(recs_content.shape[0], k=1)\n",
    "    ]\n",
    "\n",
    "    similarity = (similarity + 1) / 2\n",
    "    similarity = similarity.mean()\n",
    "\n",
    "    return 1 - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMF Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = PMF(943, 1682, 100).to(\"cuda\")\n",
    "reward_model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"../model/pmf/ml_100k_emb_100_ratio_0.800000_bs_1000_e_200_wd_0.100000_lr_0.000100_trained_pmf.pt\",\n",
    "        map_location=torch.device(\"cuda\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "user_embeddings = reward_model.user_embeddings.weight.data\n",
    "item_embeddings = reward_model.item_embeddings.weight.data\n",
    "\n",
    "users_pmf_emb = pd.DataFrame(\n",
    "    user_embeddings[users_df.index.values].cpu().numpy().tolist()\n",
    ")\n",
    "item_pmf_emb = pd.DataFrame(\n",
    "    item_embeddings[items_df.index.values].cpu().numpy().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(users_pmf_emb.values)\n",
    "# Add to dataframe for convenience\n",
    "users_pmf_emb[\"x\"] = embs[:, 0]\n",
    "users_pmf_emb[\"y\"] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(users_pmf_emb.x, users_pmf_emb.y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(item_pmf_emb.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df[\"x\"] = embs[:, 0]\n",
    "items_df[\"y\"] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(\n",
    "    items_df[items_df[\"item_name\"].str.startswith(\"Star Trek\")].index,\n",
    "    text=False,\n",
    "    alpha=0.4,\n",
    "    figsize=(15, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(\n",
    "    items_df[items_df[\"item_name\"].str.startswith(\"Star Trek\")].index.values,\n",
    "    item_pmf_emb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = (\n",
    "    ratings_df[ratings_df[\"rating\"] > 3]\n",
    "    .groupby(\"user_id\")\n",
    "    .agg({\"item_id\": lambda x: x.tolist()})\n",
    "    .reset_index()\n",
    ")\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(\n",
    "    lambda x: single_list_similarity(x, item_pmf_emb)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(users[\"user_id\"].values, users_pmf_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "fig = px.histogram(users, x=\"similarity\", template=\"ggplot2\")\n",
    "fig.update_layout(\n",
    "    title=\"Movie Lens\",\n",
    "    xaxis_range=[0, 1],\n",
    "    xaxis_title=\"User Fairness Affinity\",\n",
    "    yaxis_title=\"# Users\",\n",
    "    width=700,\n",
    "    height=400,\n",
    "    font=dict(\n",
    "        size=20,\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer(\n",
    "    \"all-MiniLM-L12-v1\"\n",
    ")  # bert-base-nli-mean-tokens / all-MiniLM-L6-v2\n",
    "sentence_embeddings = bert.encode(items_df[\"item_name\"].tolist())\n",
    "sentence_embeddings = pd.DataFrame(sentence_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(sentence_embeddings.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df[\"x\"] = embs[:, 0]\n",
    "items_df[\"y\"] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(\n",
    "    items_df[items_df[\"title\"].str.startswith(\"Star Trek\")].index,\n",
    "    text=False,\n",
    "    alpha=0.4,\n",
    "    figsize=(15, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(\n",
    "    items_df[items_df[\"title\"].str.startswith(\"Star Trek\")].index.values,\n",
    "    sentence_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = (\n",
    "    ratings_df[ratings_df[\"rating\"] > 3]\n",
    "    .groupby(\"user_id\")\n",
    "    .agg({\"item_id\": lambda x: x.tolist()})\n",
    "    .reset_index()\n",
    ")\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(\n",
    "    lambda x: single_list_similarity(x, sentence_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(users, x=\"user_id\", y=\"similarity\", title=\"Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(\n",
    "    users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_intent = items_metadata_df[items_metadata_df.index.isin([0, 1, 2, 3])]\n",
    "user_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_embeddings = items_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(genre_embeddings.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df[\"x\"] = embs[:, 0]\n",
    "items_df[\"y\"] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(\n",
    "    items_df[items_df[\"title\"].str.startswith(\"Star Trek\")].index,\n",
    "    text=False,\n",
    "    alpha=0.4,\n",
    "    figsize=(15, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(\n",
    "    items_df[items_df[\"title\"].str.startswith(\"Star Trek\")].index.values,\n",
    "    genre_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = (\n",
    "    ratings_df[ratings_df[\"rating\"] > 3]\n",
    "    .groupby(\"user_id\")\n",
    "    .agg({\"item_id\": lambda x: x.tolist()})\n",
    "    .reset_index()\n",
    ")\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(\n",
    "    lambda x: single_list_similarity(x, genre_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(users, x=\"user_id\", y=\"similarity\", title=\"Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(\n",
    "    users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title + Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\"\n",
    ")  # bert-base-nli-mean-tokens / all-MiniLM-L6-v2\n",
    "sentence_embeddings = bert.encode(items_df[\"title\"].tolist())\n",
    "sentence_embeddings = pd.DataFrame(sentence_embeddings.tolist())\n",
    "\n",
    "genre_embeddings = items_metadata_df\n",
    "\n",
    "join_embeddings = pd.concat([genre_embeddings, sentence_embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(join_embeddings.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df[\"x\"] = embs[:, 0]\n",
    "items_df[\"y\"] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(\n",
    "    items_df[items_df[\"title\"].str.startswith(\"Star Trek\")].index,\n",
    "    text=False,\n",
    "    alpha=0.4,\n",
    "    figsize=(15, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(\n",
    "    items_df[items_df[\"title\"].str.startswith(\"Star Trek\")].index.values,\n",
    "    join_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = (\n",
    "    ratings_df[ratings_df[\"rating\"] > 3]\n",
    "    .groupby(\"user_id\")\n",
    "    .agg({\"item_id\": lambda x: x.tolist()})\n",
    "    .reset_index()\n",
    ")\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(\n",
    "    lambda x: single_list_similarity(x, join_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(users, x=\"user_id\", y=\"similarity\", title=\"Similarity of User Positive Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(\n",
    "    users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../context_embedding/\")\n",
    "\n",
    "from recommenders import (\n",
    "    ContextRecommender,\n",
    "    TitleEmbeddingRecommender,\n",
    "    EmbeddingSpaceRecommender,\n",
    "    RandomRecommender,\n",
    ")\n",
    "from embedders import ContextEncoder_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = ContextRecommender(\n",
    "    model_class=ContextEncoder_v1,\n",
    "    model_folder=\"../context_embedding/movielens_runs/ContextEmbv1_5_512_Att4_Triplet_Cosine_e100_d1\",\n",
    "    data_folder=\"../context_embedding/ml-100k\",\n",
    "    item_data_file_name=\"item_data_v1.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pmf_emb = pd.DataFrame(\n",
    "    recommender.get_embedding(np.expand_dims(items_df.index.values, axis=1).tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n",
    "# out some marginal improvements. NB: This takes almost an hour!\n",
    "tsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n",
    "\n",
    "embs = tsne.fit_transform(item_pmf_emb.values)\n",
    "# Add to dataframe for convenience\n",
    "items_df[\"x\"] = embs[:, 0]\n",
    "items_df[\"y\"] = embs[:, 1]\n",
    "\n",
    "FS = (10, 8)\n",
    "fig, ax = plt.subplots(figsize=FS)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "# Make points translucent so we can visually identify regions with a high density of overlapping points\n",
    "ax.scatter(items_df.x, items_df.y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_annotations(\n",
    "    items_df[items_df[\"item_name\"].str.startswith(\"Star Trek\")].index,\n",
    "    text=False,\n",
    "    alpha=0.4,\n",
    "    figsize=(15, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list_similarity(\n",
    "    items_df[items_df[\"item_name\"].str.startswith(\"Star Trek\")].index.values,\n",
    "    item_pmf_emb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = (\n",
    "    ratings_df[ratings_df[\"rating\"] > 3]\n",
    "    .groupby(\"user_id\")\n",
    "    .agg({\"item_id\": lambda x: x.tolist()})\n",
    "    .reset_index()\n",
    ")\n",
    "users[\"similarity\"] = users[\"item_id\"].apply(\n",
    "    lambda x: single_list_similarity(x, item_pmf_emb)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"] = users[\"similarity\"].apply(lambda x: (x + 1) / 2)\n",
    "px.histogram(\n",
    "    users, x=\"similarity_norm\", title=\"Histogram of Similarity of User Positive Ratings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"similarity_norm\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.similarity_norm.describe()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23b1c047fd19bf20d4111f807c9e6bee9fdce0e396893bab828ebbcdf9a68cad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rsrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
