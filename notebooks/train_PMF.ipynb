{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys \n",
        "sys.path.append('..')\n",
        "\n",
        "from src.model.pmf import PMF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def RMSE(preds, truth):\n",
        "    return np.sqrt(np.mean(np.square(preds-truth)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 1000\n",
        "epoches = 1000\n",
        "no_cuda = False\n",
        "seed = 1\n",
        "weight_decay = 0.1\n",
        "embedding_feature_size = 50\n",
        "ratio = 0.8\n",
        "lr = 0.0001\n",
        "momentum = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0</td>\n",
              "      <td>3117</td>\n",
              "      <td>4.0</td>\n",
              "      <td>978300019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>1250</td>\n",
              "      <td>5.0</td>\n",
              "      <td>978300055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>1672</td>\n",
              "      <td>4.0</td>\n",
              "      <td>978300055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>1009</td>\n",
              "      <td>5.0</td>\n",
              "      <td>978300055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>2271</td>\n",
              "      <td>3.0</td>\n",
              "      <td>978300103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    user_id  movie_id  rating  timestamp\n",
              "31        0      3117     4.0  978300019\n",
              "22        0      1250     5.0  978300055\n",
              "27        0      1672     4.0  978300055\n",
              "37        0      1009     5.0  978300055\n",
              "24        0      2271     3.0  978300103"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rp = pd.read_csv(\"../data/ml-1m/ratings.csv\")\n",
        "rp[\"rating\"] = rp[\"rating\"].astype(\"float\")\n",
        "rp = rp.sort_values([\"user_id\", \"timestamp\"])\n",
        "rp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = rp[[\"user_id\", \"movie_id\", \"rating\"]].values\n",
        " \n",
        "# Normalize rewards to [-1, 1]\n",
        "data[:,2] = 0.5*(data[:,2] - 3)\n",
        "\n",
        "# Shuffle data\n",
        "np.random.shuffle(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "train_data = data[:int(ratio * data.shape[0])]\n",
        "vali_data = data[int(ratio * data.shape[0]): int((ratio+(1-ratio)/2)*data.shape[0])]\n",
        "test_data = data[int((ratio + (1 - ratio) / 2) * data.shape[0]) :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_USERS = 6040 # 943  \n",
        "NUM_ITEMS = 3883 # 1682 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get CUDA device if available\n",
        "cuda = torch.cuda.is_available()\n",
        " \n",
        "# Set device to CUDA or CPU, depending on availability and desire\n",
        "device = torch.device(\"cuda\" if cuda and no_cuda else \"cpu\")\n",
        " \n",
        "# Generate and apply seeds\n",
        "torch.manual_seed(seed=seed)\n",
        "if cuda:\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.manual_seed(seed=seed)\n",
        " \n",
        "# Specify number of workers for cuda\n",
        "kwargs = {\"num_workers\":1, \"pin_memory\":True} if cuda else {}\n",
        " \n",
        "# Construct Data Loaders\n",
        "train_data_loader = torch.utils.data.DataLoader(torch.from_numpy(train_data), batch_size=batch_size, shuffle=False, **kwargs)\n",
        "test_data_loader = torch.utils.data.DataLoader(torch.from_numpy(test_data), batch_size=batch_size, shuffle=False, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model moved to CUDA\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = PMF(n_users=NUM_USERS, n_items=NUM_ITEMS, n_factors=embedding_feature_size, no_cuda=no_cuda)\n",
        " \n",
        "# Move model to CUDA if CUDA selected\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    print(\"Model moved to CUDA\")\n",
        " \n",
        "# Set loss function\n",
        "loss_function = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "# Set optimizer (uncomment Adam for adam)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for training one epoch\n",
        "def train(epoch, train_data_loader):\n",
        "    # Initialize\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        " \n",
        "    # Go through batches\n",
        "    for batch_idx, ele in enumerate(train_data_loader):\n",
        "        # Zero optimizer gradient\n",
        "        optimizer.zero_grad()\n",
        " \n",
        "        # Extract user_id_nums: row 0, item_id_nums: col 1 , ratings: val 2\n",
        "        row = ele[:, 0]\n",
        "        col = ele[:, 1]\n",
        "        val = ele[:, 2]\n",
        " \n",
        "        # Set to variables\n",
        "        row = Variable(row.long())\n",
        "        if isinstance(col, list):\n",
        "            col = tuple(Variable(c.long()) for c in col)\n",
        "        else:\n",
        "            col = Variable(col.long())\n",
        "        val = Variable(val.float())\n",
        "\n",
        "        # Move data to CUDA\n",
        "        if cuda:\n",
        "            row = row.cuda()\n",
        "            col = col.cuda()\n",
        "            val = val.cuda()\n",
        " \n",
        "        # Train\n",
        "        preds = model.forward(row, col)\n",
        "        loss = loss_function(preds, val)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        " \n",
        "        # Update epoch loss\n",
        "        epoch_loss += loss.data\n",
        " \n",
        "    epoch_loss /= train_data_loader.dataset.shape[0]\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T4oQhwIIV37u",
        "outputId": "a2a0db8f-3468-45e2-8be7-20f5d3be655a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "parameters are: train ratio:0.800000,batch_size:1000, epoches:1000, weight_decay:0.100000\n",
            "PMF(\n",
            "  (user_embeddings): Embedding(6040, 50)\n",
            "  (item_embeddings): Embedding(3883, 50)\n",
            "  (ub): Embedding(6040, 1)\n",
            "  (ib): Embedding(3883, 1)\n",
            ")\n",
            "Training epoch: 1, training rmse: 0.556999, vali rmse:0.532517\n",
            "Training epoch: 2, training rmse: 0.518710, vali rmse:0.506349\n",
            "Training epoch: 3, training rmse: 0.499857, vali rmse:0.494440\n",
            "Training epoch: 4, training rmse: 0.490884, vali rmse:0.488418\n",
            "Training epoch: 5, training rmse: 0.485798, vali rmse:0.484597\n",
            "Training epoch: 6, training rmse: 0.482283, vali rmse:0.481724\n",
            "Training epoch: 7, training rmse: 0.479524, vali rmse:0.479353\n",
            "Training epoch: 8, training rmse: 0.477204, vali rmse:0.477298\n",
            "Training epoch: 9, training rmse: 0.475172, vali rmse:0.475464\n",
            "Training epoch: 10, training rmse: 0.473350, vali rmse:0.473798\n",
            "Training epoch: 11, training rmse: 0.471688, vali rmse:0.472267\n",
            "Training epoch: 12, training rmse: 0.470159, vali rmse:0.470851\n",
            "Training epoch: 13, training rmse: 0.468743, vali rmse:0.469536\n",
            "Training epoch: 14, training rmse: 0.467427, vali rmse:0.468313\n",
            "Training epoch: 15, training rmse: 0.466203, vali rmse:0.467176\n",
            "Training epoch: 16, training rmse: 0.465067, vali rmse:0.466121\n",
            "Training epoch: 17, training rmse: 0.464015, vali rmse:0.465145\n",
            "Training epoch: 18, training rmse: 0.463044, vali rmse:0.464248\n",
            "Training epoch: 19, training rmse: 0.462153, vali rmse:0.463426\n",
            "Training epoch: 20, training rmse: 0.461338, vali rmse:0.462678\n",
            "Training epoch: 21, training rmse: 0.460598, vali rmse:0.462000\n",
            "Training epoch: 22, training rmse: 0.459927, vali rmse:0.461389\n",
            "Training epoch: 23, training rmse: 0.459323, vali rmse:0.460841\n",
            "Training epoch: 24, training rmse: 0.458778, vali rmse:0.460349\n",
            "Training epoch: 25, training rmse: 0.458288, vali rmse:0.459908\n",
            "Training epoch: 26, training rmse: 0.457846, vali rmse:0.459513\n",
            "Training epoch: 27, training rmse: 0.457446, vali rmse:0.459158\n",
            "Training epoch: 28, training rmse: 0.457084, vali rmse:0.458839\n",
            "Training epoch: 29, training rmse: 0.456754, vali rmse:0.458549\n",
            "Training epoch: 30, training rmse: 0.456452, vali rmse:0.458287\n",
            "Training epoch: 31, training rmse: 0.456176, vali rmse:0.458050\n",
            "Training epoch: 32, training rmse: 0.455923, vali rmse:0.457833\n",
            "Training epoch: 33, training rmse: 0.455691, vali rmse:0.457637\n",
            "Training epoch: 34, training rmse: 0.455479, vali rmse:0.457458\n",
            "Training epoch: 35, training rmse: 0.455283, vali rmse:0.457295\n",
            "Training epoch: 36, training rmse: 0.455104, vali rmse:0.457147\n",
            "Training epoch: 37, training rmse: 0.454941, vali rmse:0.457012\n",
            "Training epoch: 38, training rmse: 0.454791, vali rmse:0.456889\n",
            "Training epoch: 39, training rmse: 0.454654, vali rmse:0.456778\n",
            "Training epoch: 40, training rmse: 0.454530, vali rmse:0.456677\n",
            "Training epoch: 41, training rmse: 0.454416, vali rmse:0.456584\n",
            "Training epoch: 42, training rmse: 0.454312, vali rmse:0.456501\n",
            "Training epoch: 43, training rmse: 0.454217, vali rmse:0.456424\n",
            "Training epoch: 44, training rmse: 0.454130, vali rmse:0.456355\n",
            "Training epoch: 45, training rmse: 0.454051, vali rmse:0.456292\n",
            "Training epoch: 46, training rmse: 0.453979, vali rmse:0.456234\n",
            "Training epoch: 47, training rmse: 0.453912, vali rmse:0.456181\n",
            "Training epoch: 48, training rmse: 0.453852, vali rmse:0.456132\n",
            "Training epoch: 49, training rmse: 0.453796, vali rmse:0.456088\n",
            "Training epoch: 50, training rmse: 0.453745, vali rmse:0.456047\n",
            "Training epoch: 51, training rmse: 0.453698, vali rmse:0.456009\n",
            "Training epoch: 52, training rmse: 0.453654, vali rmse:0.455974\n",
            "Training epoch: 53, training rmse: 0.453614, vali rmse:0.455942\n",
            "Training epoch: 54, training rmse: 0.453577, vali rmse:0.455912\n",
            "Training epoch: 55, training rmse: 0.453543, vali rmse:0.455885\n",
            "Training epoch: 56, training rmse: 0.453510, vali rmse:0.455859\n",
            "Training epoch: 57, training rmse: 0.453481, vali rmse:0.455836\n",
            "Training epoch: 58, training rmse: 0.453454, vali rmse:0.455814\n",
            "Training epoch: 59, training rmse: 0.453428, vali rmse:0.455793\n",
            "Training epoch: 60, training rmse: 0.453405, vali rmse:0.455774\n",
            "Training epoch: 61, training rmse: 0.453383, vali rmse:0.455757\n",
            "Training epoch: 62, training rmse: 0.453362, vali rmse:0.455740\n",
            "Training epoch: 63, training rmse: 0.453343, vali rmse:0.455725\n",
            "Training epoch: 64, training rmse: 0.453325, vali rmse:0.455710\n",
            "Training epoch: 65, training rmse: 0.453309, vali rmse:0.455697\n",
            "Training epoch: 66, training rmse: 0.453293, vali rmse:0.455684\n",
            "Training epoch: 67, training rmse: 0.453279, vali rmse:0.455672\n",
            "Training epoch: 68, training rmse: 0.453265, vali rmse:0.455661\n",
            "Training epoch: 69, training rmse: 0.453252, vali rmse:0.455651\n",
            "Training epoch: 70, training rmse: 0.453240, vali rmse:0.455641\n",
            "Training epoch: 71, training rmse: 0.453229, vali rmse:0.455632\n",
            "Training epoch: 72, training rmse: 0.453218, vali rmse:0.455623\n",
            "Training epoch: 73, training rmse: 0.453208, vali rmse:0.455615\n",
            "Training epoch: 74, training rmse: 0.453198, vali rmse:0.455607\n",
            "Training epoch: 75, training rmse: 0.453189, vali rmse:0.455600\n",
            "Training epoch: 76, training rmse: 0.453181, vali rmse:0.455593\n",
            "Training epoch: 77, training rmse: 0.453172, vali rmse:0.455586\n",
            "Training epoch: 78, training rmse: 0.453165, vali rmse:0.455580\n",
            "Training epoch: 79, training rmse: 0.453158, vali rmse:0.455574\n",
            "Training epoch: 80, training rmse: 0.453150, vali rmse:0.455568\n",
            "Training epoch: 81, training rmse: 0.453144, vali rmse:0.455563\n",
            "Training epoch: 82, training rmse: 0.453137, vali rmse:0.455557\n",
            "Training epoch: 83, training rmse: 0.453131, vali rmse:0.455552\n",
            "Training epoch: 84, training rmse: 0.453125, vali rmse:0.455548\n",
            "Training epoch: 85, training rmse: 0.453120, vali rmse:0.455543\n",
            "Training epoch: 86, training rmse: 0.453114, vali rmse:0.455538\n",
            "Training epoch: 87, training rmse: 0.453109, vali rmse:0.455534\n",
            "Training epoch: 88, training rmse: 0.453104, vali rmse:0.455530\n",
            "Training epoch: 89, training rmse: 0.453099, vali rmse:0.455526\n",
            "Training epoch: 90, training rmse: 0.453094, vali rmse:0.455522\n",
            "Training epoch: 91, training rmse: 0.453089, vali rmse:0.455518\n",
            "Training epoch: 92, training rmse: 0.453085, vali rmse:0.455514\n",
            "Training epoch: 93, training rmse: 0.453080, vali rmse:0.455510\n",
            "Training epoch: 94, training rmse: 0.453075, vali rmse:0.455507\n",
            "Training epoch: 95, training rmse: 0.453071, vali rmse:0.455503\n",
            "Training epoch: 96, training rmse: 0.453067, vali rmse:0.455500\n",
            "Training epoch: 97, training rmse: 0.453063, vali rmse:0.455497\n",
            "Training epoch: 98, training rmse: 0.453059, vali rmse:0.455494\n",
            "Training epoch: 99, training rmse: 0.453055, vali rmse:0.455490\n",
            "Training epoch: 100, training rmse: 0.453052, vali rmse:0.455487\n",
            "Training epoch: 101, training rmse: 0.453048, vali rmse:0.455484\n",
            "Training epoch: 102, training rmse: 0.453045, vali rmse:0.455481\n",
            "Training epoch: 103, training rmse: 0.453041, vali rmse:0.455479\n",
            "Training epoch: 104, training rmse: 0.453038, vali rmse:0.455476\n",
            "Training epoch: 105, training rmse: 0.453034, vali rmse:0.455473\n",
            "Training epoch: 106, training rmse: 0.453031, vali rmse:0.455471\n",
            "Training epoch: 107, training rmse: 0.453028, vali rmse:0.455468\n",
            "Training epoch: 108, training rmse: 0.453025, vali rmse:0.455466\n",
            "Training epoch: 109, training rmse: 0.453023, vali rmse:0.455464\n",
            "Training epoch: 110, training rmse: 0.453020, vali rmse:0.455461\n",
            "Training epoch: 111, training rmse: 0.453017, vali rmse:0.455459\n",
            "Training epoch: 112, training rmse: 0.453014, vali rmse:0.455457\n",
            "Training epoch: 113, training rmse: 0.453012, vali rmse:0.455455\n",
            "Training epoch: 114, training rmse: 0.453009, vali rmse:0.455453\n",
            "Training epoch: 115, training rmse: 0.453006, vali rmse:0.455451\n",
            "Training epoch: 116, training rmse: 0.453004, vali rmse:0.455448\n",
            "Training epoch: 117, training rmse: 0.453001, vali rmse:0.455446\n",
            "Training epoch: 118, training rmse: 0.452998, vali rmse:0.455444\n",
            "Training epoch: 119, training rmse: 0.452996, vali rmse:0.455442\n",
            "Training epoch: 120, training rmse: 0.452993, vali rmse:0.455440\n",
            "Training epoch: 121, training rmse: 0.452990, vali rmse:0.455438\n",
            "Training epoch: 122, training rmse: 0.452988, vali rmse:0.455436\n",
            "Training epoch: 123, training rmse: 0.452985, vali rmse:0.455434\n",
            "Training epoch: 124, training rmse: 0.452983, vali rmse:0.455432\n",
            "Training epoch: 125, training rmse: 0.452981, vali rmse:0.455430\n",
            "Training epoch: 126, training rmse: 0.452979, vali rmse:0.455428\n",
            "Training epoch: 127, training rmse: 0.452977, vali rmse:0.455427\n",
            "Training epoch: 128, training rmse: 0.452975, vali rmse:0.455425\n",
            "Training epoch: 129, training rmse: 0.452973, vali rmse:0.455424\n",
            "Training epoch: 130, training rmse: 0.452970, vali rmse:0.455422\n",
            "Training epoch: 131, training rmse: 0.452968, vali rmse:0.455420\n",
            "Training epoch: 132, training rmse: 0.452967, vali rmse:0.455419\n",
            "Training epoch: 133, training rmse: 0.452965, vali rmse:0.455417\n",
            "Training epoch: 134, training rmse: 0.452963, vali rmse:0.455416\n",
            "Training epoch: 135, training rmse: 0.452961, vali rmse:0.455415\n",
            "Training epoch: 136, training rmse: 0.452960, vali rmse:0.455413\n",
            "Training epoch: 137, training rmse: 0.452958, vali rmse:0.455412\n",
            "Training epoch: 138, training rmse: 0.452956, vali rmse:0.455411\n",
            "Training epoch: 139, training rmse: 0.452955, vali rmse:0.455410\n",
            "Training epoch: 140, training rmse: 0.452953, vali rmse:0.455409\n",
            "Training epoch: 141, training rmse: 0.452952, vali rmse:0.455408\n",
            "Training epoch: 142, training rmse: 0.452951, vali rmse:0.455407\n",
            "Training epoch: 143, training rmse: 0.452949, vali rmse:0.455406\n",
            "Training epoch: 144, training rmse: 0.452948, vali rmse:0.455405\n",
            "Training epoch: 145, training rmse: 0.452947, vali rmse:0.455404\n",
            "Training epoch: 146, training rmse: 0.452946, vali rmse:0.455403\n",
            "Training epoch: 147, training rmse: 0.452944, vali rmse:0.455402\n",
            "Training epoch: 148, training rmse: 0.452943, vali rmse:0.455401\n",
            "Training epoch: 149, training rmse: 0.452942, vali rmse:0.455400\n",
            "Training epoch: 150, training rmse: 0.452940, vali rmse:0.455399\n",
            "Training epoch: 151, training rmse: 0.452939, vali rmse:0.455398\n",
            "Training epoch: 152, training rmse: 0.452937, vali rmse:0.455397\n",
            "Training epoch: 153, training rmse: 0.452936, vali rmse:0.455396\n",
            "Training epoch: 154, training rmse: 0.452934, vali rmse:0.455395\n",
            "Training epoch: 155, training rmse: 0.452933, vali rmse:0.455394\n",
            "Training epoch: 156, training rmse: 0.452931, vali rmse:0.455393\n",
            "Training epoch: 157, training rmse: 0.452929, vali rmse:0.455391\n",
            "Training epoch: 158, training rmse: 0.452928, vali rmse:0.455390\n",
            "Training epoch: 159, training rmse: 0.452926, vali rmse:0.455389\n",
            "Training epoch: 160, training rmse: 0.452924, vali rmse:0.455387\n",
            "Training epoch: 161, training rmse: 0.452923, vali rmse:0.455386\n",
            "Training epoch: 162, training rmse: 0.452921, vali rmse:0.455385\n",
            "Training epoch: 163, training rmse: 0.452919, vali rmse:0.455384\n",
            "Training epoch: 164, training rmse: 0.452918, vali rmse:0.455382\n",
            "Training epoch: 165, training rmse: 0.452916, vali rmse:0.455381\n",
            "Training epoch: 166, training rmse: 0.452915, vali rmse:0.455380\n",
            "Training epoch: 167, training rmse: 0.452913, vali rmse:0.455379\n",
            "Training epoch: 168, training rmse: 0.452912, vali rmse:0.455378\n",
            "Training epoch: 169, training rmse: 0.452910, vali rmse:0.455377\n",
            "Training epoch: 170, training rmse: 0.452909, vali rmse:0.455376\n",
            "Training epoch: 171, training rmse: 0.452907, vali rmse:0.455375\n",
            "Training epoch: 172, training rmse: 0.452906, vali rmse:0.455374\n",
            "Training epoch: 173, training rmse: 0.452905, vali rmse:0.455373\n",
            "Training epoch: 174, training rmse: 0.452903, vali rmse:0.455372\n",
            "Training epoch: 175, training rmse: 0.452902, vali rmse:0.455371\n",
            "Training epoch: 176, training rmse: 0.452900, vali rmse:0.455370\n",
            "Training epoch: 177, training rmse: 0.452899, vali rmse:0.455369\n",
            "Training epoch: 178, training rmse: 0.452897, vali rmse:0.455368\n",
            "Training epoch: 179, training rmse: 0.452896, vali rmse:0.455367\n",
            "Training epoch: 180, training rmse: 0.452894, vali rmse:0.455366\n",
            "Training epoch: 181, training rmse: 0.452893, vali rmse:0.455365\n",
            "Training epoch: 182, training rmse: 0.452892, vali rmse:0.455364\n",
            "Training epoch: 183, training rmse: 0.452890, vali rmse:0.455363\n",
            "Training epoch: 184, training rmse: 0.452888, vali rmse:0.455361\n",
            "Training epoch: 185, training rmse: 0.452887, vali rmse:0.455360\n",
            "Training epoch: 186, training rmse: 0.452885, vali rmse:0.455359\n",
            "Training epoch: 187, training rmse: 0.452883, vali rmse:0.455357\n",
            "Training epoch: 188, training rmse: 0.452882, vali rmse:0.455356\n",
            "Training epoch: 189, training rmse: 0.452879, vali rmse:0.455355\n",
            "Training epoch: 190, training rmse: 0.452877, vali rmse:0.455353\n",
            "Training epoch: 191, training rmse: 0.452875, vali rmse:0.455351\n",
            "Training epoch: 192, training rmse: 0.452873, vali rmse:0.455350\n",
            "Training epoch: 193, training rmse: 0.452871, vali rmse:0.455348\n",
            "Training epoch: 194, training rmse: 0.452869, vali rmse:0.455347\n",
            "Training epoch: 195, training rmse: 0.452867, vali rmse:0.455345\n",
            "Training epoch: 196, training rmse: 0.452865, vali rmse:0.455343\n",
            "Training epoch: 197, training rmse: 0.452863, vali rmse:0.455342\n",
            "Training epoch: 198, training rmse: 0.452861, vali rmse:0.455340\n",
            "Training epoch: 199, training rmse: 0.452859, vali rmse:0.455339\n",
            "Training epoch: 200, training rmse: 0.452857, vali rmse:0.455337\n",
            "Training epoch: 201, training rmse: 0.452855, vali rmse:0.455335\n",
            "Training epoch: 202, training rmse: 0.452853, vali rmse:0.455334\n",
            "Training epoch: 203, training rmse: 0.452851, vali rmse:0.455332\n",
            "Training epoch: 204, training rmse: 0.452849, vali rmse:0.455331\n",
            "Training epoch: 205, training rmse: 0.452847, vali rmse:0.455329\n",
            "Training epoch: 206, training rmse: 0.452845, vali rmse:0.455327\n",
            "Training epoch: 207, training rmse: 0.452843, vali rmse:0.455326\n",
            "Training epoch: 208, training rmse: 0.452841, vali rmse:0.455324\n",
            "Training epoch: 209, training rmse: 0.452839, vali rmse:0.455323\n",
            "Training epoch: 210, training rmse: 0.452837, vali rmse:0.455321\n",
            "Training epoch: 211, training rmse: 0.452836, vali rmse:0.455320\n",
            "Training epoch: 212, training rmse: 0.452834, vali rmse:0.455318\n",
            "Training epoch: 213, training rmse: 0.452833, vali rmse:0.455317\n",
            "Training epoch: 214, training rmse: 0.452831, vali rmse:0.455316\n",
            "Training epoch: 215, training rmse: 0.452830, vali rmse:0.455315\n",
            "Training epoch: 216, training rmse: 0.452828, vali rmse:0.455314\n",
            "Training epoch: 217, training rmse: 0.452827, vali rmse:0.455312\n",
            "Training epoch: 218, training rmse: 0.452826, vali rmse:0.455311\n",
            "Training epoch: 219, training rmse: 0.452825, vali rmse:0.455310\n",
            "Training epoch: 220, training rmse: 0.452824, vali rmse:0.455310\n",
            "Training epoch: 221, training rmse: 0.452823, vali rmse:0.455309\n",
            "Training epoch: 222, training rmse: 0.452822, vali rmse:0.455308\n",
            "Training epoch: 223, training rmse: 0.452821, vali rmse:0.455307\n",
            "Training epoch: 224, training rmse: 0.452820, vali rmse:0.455307\n",
            "Training epoch: 225, training rmse: 0.452819, vali rmse:0.455306\n",
            "Training epoch: 226, training rmse: 0.452819, vali rmse:0.455306\n",
            "Training epoch: 227, training rmse: 0.452818, vali rmse:0.455305\n",
            "Training epoch: 228, training rmse: 0.452817, vali rmse:0.455305\n",
            "Training epoch: 229, training rmse: 0.452817, vali rmse:0.455304\n",
            "Training epoch: 230, training rmse: 0.452816, vali rmse:0.455304\n",
            "Training epoch: 231, training rmse: 0.452816, vali rmse:0.455304\n",
            "Training epoch: 232, training rmse: 0.452816, vali rmse:0.455303\n",
            "Training epoch: 233, training rmse: 0.452815, vali rmse:0.455303\n",
            "Training epoch: 234, training rmse: 0.452815, vali rmse:0.455303\n",
            "Training epoch: 235, training rmse: 0.452814, vali rmse:0.455303\n",
            "Training epoch: 236, training rmse: 0.452814, vali rmse:0.455302\n",
            "Training epoch: 237, training rmse: 0.452814, vali rmse:0.455302\n",
            "Training epoch: 238, training rmse: 0.452814, vali rmse:0.455302\n",
            "Training epoch: 239, training rmse: 0.452813, vali rmse:0.455302\n",
            "Training epoch: 240, training rmse: 0.452813, vali rmse:0.455302\n",
            "Training epoch: 241, training rmse: 0.452813, vali rmse:0.455302\n",
            "Training epoch: 242, training rmse: 0.452813, vali rmse:0.455302\n",
            "Training epoch: 243, training rmse: 0.452812, vali rmse:0.455301\n",
            "Training epoch: 244, training rmse: 0.452812, vali rmse:0.455301\n",
            "Training epoch: 245, training rmse: 0.452812, vali rmse:0.455301\n",
            "Training epoch: 246, training rmse: 0.452812, vali rmse:0.455301\n",
            "Training epoch: 247, training rmse: 0.452812, vali rmse:0.455301\n",
            "Training epoch: 248, training rmse: 0.452812, vali rmse:0.455301\n",
            "Training epoch: 249, training rmse: 0.452811, vali rmse:0.455301\n",
            "Training epoch: 250, training rmse: 0.452811, vali rmse:0.455301\n",
            "Training epoch: 251, training rmse: 0.452811, vali rmse:0.455301\n",
            "Training epoch: 252, training rmse: 0.452811, vali rmse:0.455301\n",
            "Training epoch: 253, training rmse: 0.452811, vali rmse:0.455301\n",
            "Training epoch: 254, training rmse: 0.452810, vali rmse:0.455301\n",
            "Training epoch: 255, training rmse: 0.452810, vali rmse:0.455300\n",
            "Training epoch: 256, training rmse: 0.452810, vali rmse:0.455300\n",
            "Training epoch: 257, training rmse: 0.452810, vali rmse:0.455300\n",
            "Training epoch: 258, training rmse: 0.452810, vali rmse:0.455300\n",
            "Training epoch: 259, training rmse: 0.452809, vali rmse:0.455300\n",
            "Training epoch: 260, training rmse: 0.452809, vali rmse:0.455300\n",
            "Training epoch: 261, training rmse: 0.452809, vali rmse:0.455300\n",
            "Training epoch: 262, training rmse: 0.452809, vali rmse:0.455300\n",
            "Training epoch: 263, training rmse: 0.452808, vali rmse:0.455299\n",
            "Training epoch: 264, training rmse: 0.452808, vali rmse:0.455299\n",
            "Training epoch: 265, training rmse: 0.452808, vali rmse:0.455299\n",
            "Training epoch: 266, training rmse: 0.452807, vali rmse:0.455298\n",
            "Training epoch: 267, training rmse: 0.452807, vali rmse:0.455298\n",
            "Training epoch: 268, training rmse: 0.452806, vali rmse:0.455298\n",
            "Training epoch: 269, training rmse: 0.452806, vali rmse:0.455297\n",
            "Training epoch: 270, training rmse: 0.452805, vali rmse:0.455297\n",
            "Training epoch: 271, training rmse: 0.452805, vali rmse:0.455297\n",
            "Training epoch: 272, training rmse: 0.452804, vali rmse:0.455296\n",
            "Training epoch: 273, training rmse: 0.452804, vali rmse:0.455296\n",
            "Training epoch: 274, training rmse: 0.452803, vali rmse:0.455296\n",
            "Training epoch: 275, training rmse: 0.452803, vali rmse:0.455296\n",
            "Training epoch: 276, training rmse: 0.452803, vali rmse:0.455295\n",
            "Training epoch: 277, training rmse: 0.452802, vali rmse:0.455295\n",
            "Training epoch: 278, training rmse: 0.452802, vali rmse:0.455295\n",
            "Training epoch: 279, training rmse: 0.452802, vali rmse:0.455295\n",
            "Training epoch: 280, training rmse: 0.452802, vali rmse:0.455295\n",
            "Training epoch: 281, training rmse: 0.452801, vali rmse:0.455295\n",
            "Training epoch: 282, training rmse: 0.452801, vali rmse:0.455295\n",
            "Training epoch: 283, training rmse: 0.452801, vali rmse:0.455295\n",
            "Training epoch: 284, training rmse: 0.452801, vali rmse:0.455294\n",
            "Training epoch: 285, training rmse: 0.452801, vali rmse:0.455294\n",
            "Training epoch: 286, training rmse: 0.452800, vali rmse:0.455294\n",
            "Training epoch: 287, training rmse: 0.452800, vali rmse:0.455294\n",
            "Training epoch: 288, training rmse: 0.452800, vali rmse:0.455294\n",
            "Training epoch: 289, training rmse: 0.452800, vali rmse:0.455294\n",
            "Training epoch: 290, training rmse: 0.452800, vali rmse:0.455294\n",
            "Training epoch: 291, training rmse: 0.452799, vali rmse:0.455294\n",
            "Training epoch: 292, training rmse: 0.452799, vali rmse:0.455294\n",
            "Training epoch: 293, training rmse: 0.452799, vali rmse:0.455294\n",
            "Training epoch: 294, training rmse: 0.452799, vali rmse:0.455294\n",
            "Training epoch: 295, training rmse: 0.452798, vali rmse:0.455294\n",
            "Training epoch: 296, training rmse: 0.452798, vali rmse:0.455294\n",
            "Training epoch: 297, training rmse: 0.452798, vali rmse:0.455293\n",
            "Training epoch: 298, training rmse: 0.452798, vali rmse:0.455293\n",
            "Training epoch: 299, training rmse: 0.452797, vali rmse:0.455293\n",
            "Training epoch: 300, training rmse: 0.452797, vali rmse:0.455293\n",
            "Training epoch: 301, training rmse: 0.452796, vali rmse:0.455293\n",
            "Training epoch: 302, training rmse: 0.452796, vali rmse:0.455292\n",
            "Training epoch: 303, training rmse: 0.452795, vali rmse:0.455292\n",
            "Training epoch: 304, training rmse: 0.452795, vali rmse:0.455292\n",
            "Training epoch: 305, training rmse: 0.452794, vali rmse:0.455291\n",
            "Training epoch: 306, training rmse: 0.452794, vali rmse:0.455291\n",
            "Training epoch: 307, training rmse: 0.452793, vali rmse:0.455290\n",
            "Training epoch: 308, training rmse: 0.452792, vali rmse:0.455290\n",
            "Training epoch: 309, training rmse: 0.452791, vali rmse:0.455289\n",
            "Training epoch: 310, training rmse: 0.452790, vali rmse:0.455288\n",
            "Training epoch: 311, training rmse: 0.452789, vali rmse:0.455287\n",
            "Training epoch: 312, training rmse: 0.452788, vali rmse:0.455286\n",
            "Training epoch: 313, training rmse: 0.452787, vali rmse:0.455285\n",
            "Training epoch: 314, training rmse: 0.452785, vali rmse:0.455284\n",
            "Training epoch: 315, training rmse: 0.452784, vali rmse:0.455283\n",
            "Training epoch: 316, training rmse: 0.452783, vali rmse:0.455282\n",
            "Training epoch: 317, training rmse: 0.452782, vali rmse:0.455282\n",
            "Training epoch: 318, training rmse: 0.452781, vali rmse:0.455281\n",
            "Training epoch: 319, training rmse: 0.452779, vali rmse:0.455280\n",
            "Training epoch: 320, training rmse: 0.452778, vali rmse:0.455279\n",
            "Training epoch: 321, training rmse: 0.452777, vali rmse:0.455278\n",
            "Training epoch: 322, training rmse: 0.452776, vali rmse:0.455277\n",
            "Training epoch: 323, training rmse: 0.452775, vali rmse:0.455276\n",
            "Training epoch: 324, training rmse: 0.452774, vali rmse:0.455275\n",
            "Training epoch: 325, training rmse: 0.452773, vali rmse:0.455275\n",
            "Training epoch: 326, training rmse: 0.452773, vali rmse:0.455274\n",
            "Training epoch: 327, training rmse: 0.452772, vali rmse:0.455273\n",
            "Training epoch: 328, training rmse: 0.452771, vali rmse:0.455273\n",
            "Training epoch: 329, training rmse: 0.452770, vali rmse:0.455272\n",
            "Training epoch: 330, training rmse: 0.452770, vali rmse:0.455272\n",
            "Training epoch: 331, training rmse: 0.452769, vali rmse:0.455271\n",
            "Training epoch: 332, training rmse: 0.452769, vali rmse:0.455271\n",
            "Training epoch: 333, training rmse: 0.452768, vali rmse:0.455271\n",
            "Training epoch: 334, training rmse: 0.452768, vali rmse:0.455270\n",
            "Training epoch: 335, training rmse: 0.452767, vali rmse:0.455270\n",
            "Training epoch: 336, training rmse: 0.452767, vali rmse:0.455270\n",
            "Training epoch: 337, training rmse: 0.452766, vali rmse:0.455269\n",
            "Training epoch: 338, training rmse: 0.452766, vali rmse:0.455269\n",
            "Training epoch: 339, training rmse: 0.452766, vali rmse:0.455269\n",
            "Training epoch: 340, training rmse: 0.452765, vali rmse:0.455268\n",
            "Training epoch: 341, training rmse: 0.452765, vali rmse:0.455268\n",
            "Training epoch: 342, training rmse: 0.452765, vali rmse:0.455268\n",
            "Training epoch: 343, training rmse: 0.452764, vali rmse:0.455268\n",
            "Training epoch: 344, training rmse: 0.452764, vali rmse:0.455268\n",
            "Training epoch: 345, training rmse: 0.452764, vali rmse:0.455267\n",
            "Training epoch: 346, training rmse: 0.452763, vali rmse:0.455267\n",
            "Training epoch: 347, training rmse: 0.452763, vali rmse:0.455267\n",
            "Training epoch: 348, training rmse: 0.452763, vali rmse:0.455267\n",
            "Training epoch: 349, training rmse: 0.452762, vali rmse:0.455267\n",
            "Training epoch: 350, training rmse: 0.452762, vali rmse:0.455267\n",
            "Training epoch: 351, training rmse: 0.452762, vali rmse:0.455266\n",
            "Training epoch: 352, training rmse: 0.452762, vali rmse:0.455266\n",
            "Training epoch: 353, training rmse: 0.452761, vali rmse:0.455266\n",
            "Training epoch: 354, training rmse: 0.452761, vali rmse:0.455266\n",
            "Training epoch: 355, training rmse: 0.452761, vali rmse:0.455265\n",
            "Training epoch: 356, training rmse: 0.452760, vali rmse:0.455265\n",
            "Training epoch: 357, training rmse: 0.452760, vali rmse:0.455265\n",
            "Training epoch: 358, training rmse: 0.452760, vali rmse:0.455265\n",
            "Training epoch: 359, training rmse: 0.452759, vali rmse:0.455264\n",
            "Training epoch: 360, training rmse: 0.452759, vali rmse:0.455264\n",
            "Training epoch: 361, training rmse: 0.452758, vali rmse:0.455263\n",
            "Training epoch: 362, training rmse: 0.452757, vali rmse:0.455263\n",
            "Training epoch: 363, training rmse: 0.452757, vali rmse:0.455262\n",
            "Training epoch: 364, training rmse: 0.452756, vali rmse:0.455262\n",
            "Training epoch: 365, training rmse: 0.452756, vali rmse:0.455261\n",
            "Training epoch: 366, training rmse: 0.452755, vali rmse:0.455261\n",
            "Training epoch: 367, training rmse: 0.452754, vali rmse:0.455261\n",
            "Training epoch: 368, training rmse: 0.452754, vali rmse:0.455260\n",
            "Training epoch: 369, training rmse: 0.452754, vali rmse:0.455260\n",
            "Training epoch: 370, training rmse: 0.452753, vali rmse:0.455260\n",
            "Training epoch: 371, training rmse: 0.452753, vali rmse:0.455259\n",
            "Training epoch: 372, training rmse: 0.452752, vali rmse:0.455259\n",
            "Training epoch: 373, training rmse: 0.452752, vali rmse:0.455259\n",
            "Training epoch: 374, training rmse: 0.452752, vali rmse:0.455259\n",
            "Training epoch: 375, training rmse: 0.452752, vali rmse:0.455258\n",
            "Training epoch: 376, training rmse: 0.452751, vali rmse:0.455258\n",
            "Training epoch: 377, training rmse: 0.452751, vali rmse:0.455258\n",
            "Training epoch: 378, training rmse: 0.452751, vali rmse:0.455258\n",
            "Training epoch: 379, training rmse: 0.452750, vali rmse:0.455258\n",
            "Training epoch: 380, training rmse: 0.452750, vali rmse:0.455257\n",
            "Training epoch: 381, training rmse: 0.452750, vali rmse:0.455257\n",
            "Training epoch: 382, training rmse: 0.452750, vali rmse:0.455257\n",
            "Training epoch: 383, training rmse: 0.452750, vali rmse:0.455257\n",
            "Training epoch: 384, training rmse: 0.452749, vali rmse:0.455257\n",
            "Training epoch: 385, training rmse: 0.452749, vali rmse:0.455257\n",
            "Training epoch: 386, training rmse: 0.452749, vali rmse:0.455257\n",
            "Training epoch: 387, training rmse: 0.452749, vali rmse:0.455257\n",
            "Training epoch: 388, training rmse: 0.452749, vali rmse:0.455256\n",
            "Training epoch: 389, training rmse: 0.452748, vali rmse:0.455256\n",
            "Training epoch: 390, training rmse: 0.452748, vali rmse:0.455256\n",
            "Training epoch: 391, training rmse: 0.452748, vali rmse:0.455256\n",
            "Training epoch: 392, training rmse: 0.452748, vali rmse:0.455256\n",
            "Training epoch: 393, training rmse: 0.452748, vali rmse:0.455256\n",
            "Training epoch: 394, training rmse: 0.452747, vali rmse:0.455256\n",
            "Training epoch: 395, training rmse: 0.452747, vali rmse:0.455256\n",
            "Training epoch: 396, training rmse: 0.452747, vali rmse:0.455256\n",
            "Training epoch: 397, training rmse: 0.452747, vali rmse:0.455255\n",
            "Training epoch: 398, training rmse: 0.452746, vali rmse:0.455255\n",
            "Training epoch: 399, training rmse: 0.452746, vali rmse:0.455255\n",
            "Training epoch: 400, training rmse: 0.452746, vali rmse:0.455255\n",
            "Training epoch: 401, training rmse: 0.452746, vali rmse:0.455255\n",
            "Training epoch: 402, training rmse: 0.452745, vali rmse:0.455254\n",
            "Training epoch: 403, training rmse: 0.452745, vali rmse:0.455254\n",
            "Training epoch: 404, training rmse: 0.452745, vali rmse:0.455254\n",
            "Training epoch: 405, training rmse: 0.452744, vali rmse:0.455253\n",
            "Training epoch: 406, training rmse: 0.452744, vali rmse:0.455253\n",
            "Training epoch: 407, training rmse: 0.452743, vali rmse:0.455253\n",
            "Training epoch: 408, training rmse: 0.452743, vali rmse:0.455252\n",
            "Training epoch: 409, training rmse: 0.452742, vali rmse:0.455252\n",
            "Training epoch: 410, training rmse: 0.452742, vali rmse:0.455251\n",
            "Training epoch: 411, training rmse: 0.452741, vali rmse:0.455251\n",
            "Training epoch: 412, training rmse: 0.452740, vali rmse:0.455250\n",
            "Training epoch: 413, training rmse: 0.452740, vali rmse:0.455250\n",
            "Training epoch: 414, training rmse: 0.452739, vali rmse:0.455250\n",
            "Training epoch: 415, training rmse: 0.452739, vali rmse:0.455249\n",
            "Training epoch: 416, training rmse: 0.452739, vali rmse:0.455249\n",
            "Training epoch: 417, training rmse: 0.452738, vali rmse:0.455249\n",
            "Training epoch: 418, training rmse: 0.452738, vali rmse:0.455248\n",
            "Training epoch: 419, training rmse: 0.452738, vali rmse:0.455248\n",
            "Training epoch: 420, training rmse: 0.452737, vali rmse:0.455248\n",
            "Training epoch: 421, training rmse: 0.452737, vali rmse:0.455248\n",
            "Training epoch: 422, training rmse: 0.452737, vali rmse:0.455247\n",
            "Training epoch: 423, training rmse: 0.452736, vali rmse:0.455247\n",
            "Training epoch: 424, training rmse: 0.452736, vali rmse:0.455247\n",
            "Training epoch: 425, training rmse: 0.452736, vali rmse:0.455247\n",
            "Training epoch: 426, training rmse: 0.452736, vali rmse:0.455247\n",
            "Training epoch: 427, training rmse: 0.452736, vali rmse:0.455247\n",
            "Training epoch: 428, training rmse: 0.452736, vali rmse:0.455246\n",
            "Training epoch: 429, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 430, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 431, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 432, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 433, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 434, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 435, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 436, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 437, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 438, training rmse: 0.452735, vali rmse:0.455246\n",
            "Training epoch: 439, training rmse: 0.452734, vali rmse:0.455246\n",
            "Training epoch: 440, training rmse: 0.452734, vali rmse:0.455246\n",
            "Training epoch: 441, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 442, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 443, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 444, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 445, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 446, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 447, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 448, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 449, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 450, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 451, training rmse: 0.452734, vali rmse:0.455245\n",
            "Training epoch: 452, training rmse: 0.452734, vali rmse:0.455245\n"
          ]
        }
      ],
      "source": [
        "# Training Model\n",
        "\n",
        "train_loss_list = []\n",
        "last_vali_rmse = None\n",
        "train_rmse_list = []\n",
        "vali_rmse_list = []\n",
        "print(\"parameters are: train ratio:{:f},batch_size:{:d}, epoches:{:d}, weight_decay:{:f}\".format(ratio, batch_size, epoches, weight_decay))\n",
        "print(model)\n",
        "\n",
        "# Go through epochs\n",
        "for epoch in range(1, epoches+1):\n",
        " \n",
        "    # Train epoch\n",
        "    train_epoch_loss = train(epoch, train_data_loader)\n",
        " \n",
        "    # Get epoch loss\n",
        "    train_loss_list.append(train_epoch_loss.cpu())\n",
        " \n",
        "    # Move validation data to CUDA\n",
        "    if cuda:\n",
        "        vali_row = Variable(torch.from_numpy(vali_data[:, 0]).long()).cuda()\n",
        "        vali_col = Variable(torch.from_numpy(vali_data[:, 1]).long()).cuda()\n",
        "    else:\n",
        "        vali_row = Variable(torch.from_numpy(vali_data[:, 0]).long())\n",
        "        vali_col = Variable(torch.from_numpy(vali_data[:, 1]).long())\n",
        " \n",
        "    # Get validation predictions\n",
        "    vali_preds = model.predict(vali_row, vali_col)\n",
        " \n",
        "    # Calculate train rmse loss\n",
        "    train_rmse = np.sqrt(train_epoch_loss.cpu())\n",
        " \n",
        "    # Calculate validation rmse loss\n",
        "    if cuda:\n",
        "        vali_rmse = RMSE(vali_preds.cpu().data.numpy(), vali_data[:, 2])\n",
        "    else:\n",
        "        vali_rmse = RMSE(vali_preds.data.numpy(), vali_data[:, 2])\n",
        " \n",
        "    # Add losses to rmse loss lists\n",
        "    train_rmse_list.append(train_rmse)\n",
        "    vali_rmse_list.append(vali_rmse)\n",
        " \n",
        "    print(\"Training epoch:{: d}, training rmse:{: .6f}, vali rmse:{:.6f}\". \\\n",
        "              format(epoch, train_rmse, vali_rmse))\n",
        " \n",
        "    # Early stop condition\n",
        "    if last_vali_rmse and last_vali_rmse < vali_rmse:\n",
        "        break\n",
        "    else:\n",
        "      last_vali_rmse = vali_rmse\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test rmse: 0.455068\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Testing Model\n",
        "\n",
        "# Move test set to CUDA\n",
        "if cuda:\n",
        "    test_row = Variable(torch.from_numpy(test_data[:, 0]).long()).cuda()\n",
        "    test_col = Variable(torch.from_numpy(test_data[:, 1]).long()).cuda()\n",
        "else:\n",
        "    test_row = Variable(torch.from_numpy(test_data[:, 0]).long())\n",
        "    test_col = Variable(torch.from_numpy(test_data[:, 1]).long())\n",
        " \n",
        "# Get test predictions\n",
        "preds = model.predict(test_row, test_col)\n",
        " \n",
        "# Get test rmse loss\n",
        "if cuda:\n",
        "    test_rmse = RMSE(preds.cpu().data.numpy(), test_data[:, 2])\n",
        "else:\n",
        "    test_rmse = RMSE(preds.data.numpy(), test_data[:, 2])\n",
        "print(\"Test rmse: {:f}\".format(test_rmse))\n",
        " \n",
        "# Create plots\n",
        "plt.figure(1)\n",
        "plt.plot(range(1, len(train_rmse_list)+1), train_rmse_list, color=\"r\", label=\"train rmse\")\n",
        "plt.plot(range(1, len(vali_rmse_list)+1), vali_rmse_list, color=\"b\", label=\"test rmse\")\n",
        "plt.legend()\n",
        "plt.annotate(r\"train=%f\" % (train_rmse_list[-1]), xy=(len(train_rmse_list), train_rmse_list[-1]),\n",
        "             xycoords=\"data\", xytext=(-30, 30), textcoords=\"offset points\", fontsize=10,\n",
        "             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=.2\"))\n",
        "plt.annotate(r\"vali=%f\" % (vali_rmse_list[-1]), xy=(len(vali_rmse_list), vali_rmse_list[-1]),\n",
        "             xycoords=\"data\", xytext=(-30, 30), textcoords=\"offset points\", fontsize=10,\n",
        "             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=.2\"))\n",
        "plt.xlim([1, len(train_rmse_list)+10])\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"RMSE Curve in Training Process\")\n",
        "plt.show()\n",
        " \n",
        "# Save model\n",
        "path_to_trained_pmf = \"../model/pmf/emb_{:d}_ratio_{:f}_bs_{:d}_e_{:d}_wd_{:f}_lr_{:f}_trained_pmf.pt\".format(embedding_feature_size, ratio, batch_size, len(train_rmse_list), weight_decay, lr)\n",
        "torch.save(model.state_dict(), path_to_trained_pmf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.0238)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx =  0 \n",
        "model.predict(\n",
        "    torch.tensor([data[idx][0]]).long().to(\"cuda\"), \n",
        "    torch.tensor([data[idx][1]]).long().to(\"cuda\")\n",
        ").cpu().data[0] * 2 + 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[idx][2] * 2 + 3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_PMF_project.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8842d3533bae659e5f41b6e8512932590b59a54a4b7636cc41bd679d9b5f82e4"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('recsysrl': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
