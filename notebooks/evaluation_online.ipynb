{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Evaluation\n",
    "\n",
    "We pretrain a PMF model as the environment simulator, i.e., to predict an item's feedback that the user never rates before. The online evaluation procedure follows the Training Algorithm, i.e., the parameters continuously update during the online evaluation stage. Its major difference is that the feedback of a recommended item is observed by the environment simulator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1NmejTIFPtf"
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "#Dependencies\n",
    "import os\n",
    "import json \n",
    "import yaml\n",
    "import pickle\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.environment import OfflineEnv, OfflineFairEnv\n",
    "from src.model.recommender import DRRAgent, FairRecAgent\n",
    "\n",
    "from src.recsys_fair_metrics.recsys_fair import RecsysFair\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import recmetrics as rm\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "ENV = dict(drr=OfflineEnv, fairrec=OfflineFairEnv)\n",
    "AGENT = dict(drr=DRRAgent, fairrec=FairRecAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"movie_lens_1m\"\n",
    "dataset_path = \"../data/{}_output_path.json\".format(dataset_name)\n",
    "with open(dataset_path) as json_file:\n",
    "    _dataset_path = json.load(json_file)\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "with open(os.path.join(\"..\", _dataset_path[\"eval_users_dict\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"eval_users_dict\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"eval_users_history_lens\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"eval_users_history_lens\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"users_history_lens\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"users_history_lens\"] = pickle.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(\"..\", _dataset_path[\"item_groups\"]), \"rb\") as pkl_file:\n",
    "    dataset[\"item_groups\"] = pickle.load(pkl_file)\n",
    "\n",
    "item_groups_df = pd.DataFrame(dataset[\"item_groups\"].items(), columns=[\"item_id\", \"group\"])\n",
    "catalog = item_groups_df.item_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drr_train_ids = {\n",
    "    0: \"movie_lens_1m_2022-04-05_14-47-30\", # ou noise sem reset\n",
    "    1: \"movie_lens_1m_2022-04-05_16-33-30\", # com reset\n",
    "    2: \"movie_lens_1m_2022-04-06_11-29-07\", # drr paper\n",
    "}\n",
    "\n",
    "fairrec_train_ids = {   \n",
    "    # Paper\n",
    "    0: \"movie_lens_1m_fair_2022-04-05_14-51-24\",\n",
    "    1: \"movie_lens_1m_fair_2022-04-05_21-33-12\",\n",
    "\n",
    "    # Adaptative\n",
    "    2: \"movie_lens_1m_fair_2022-04-06_09-10-12\",\n",
    "   \n",
    "    # Combining\n",
    "    3: \"movie_lens_1m_fair_2022-04-05_23-46-04\",\n",
    "   \n",
    "}\n",
    "\n",
    "idx = 2\n",
    "\n",
    "\n",
    "algorithm = \"drr\"\n",
    "train_version = dataset_name if algorithm == \"drr\" else \"{}_fair\".format(dataset_name)\n",
    "train_id = drr_train_ids[idx] if algorithm == \"drr\" else fairrec_train_ids[idx]\n",
    "output_path = \"../model/{}/{}\".format(train_version, train_id)\n",
    "\n",
    "path = os.path.abspath(\n",
    "    os.path.join(output_path, \"{}.yaml\".format(train_version))\n",
    ")\n",
    "with open(path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "no_cuda = False\n",
    "top_k = [3, 5, 10, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_precision = []\n",
    "_propfair = []\n",
    "_ufg = []\n",
    "_recommended_item = []\n",
    "_random_recommended_item = []\n",
    "for k in top_k:\n",
    "    sum_precision = 0\n",
    "    sum_propfair = 0\n",
    "    sum_reward = 0\n",
    "\n",
    "    recommended_item = []\n",
    "    random_recommended_item = []\n",
    "\n",
    "    env = ENV[algorithm](\n",
    "        users_dict=dataset[\"eval_users_dict\"],\n",
    "        users_history_lens=dataset[\"eval_users_history_lens\"],\n",
    "        n_groups=config[\"model_train\"][\"n_groups\"],\n",
    "        item_groups=dataset[\"item_groups\"],\n",
    "        state_size=config[\"model_train\"][\"state_size\"],\n",
    "        done_count=k,# config[\"model_train\"][\"done_count\"],\n",
    "        fairness_constraints=config[\"model_train\"][\"fairness_constraints\"],\n",
    "        reward_threshold=config[\"model_train\"][\"reward_threshold\"],\n",
    "        reward_version=config[\"model_train\"][\"reward_version\"],\n",
    "        use_only_reward_model=True,\n",
    "    )\n",
    "    available_users = env.available_users\n",
    "\n",
    "    recommender = AGENT[algorithm](\n",
    "        env=env,\n",
    "        is_test=True,\n",
    "        train_version=\"{}_{}\".format(train_version, config[\"model_train\"][\"reward_version\"]),\n",
    "        model_path=output_path,\n",
    "        users_num=config[\"model_train\"][\"users_num\"],\n",
    "        items_num=config[\"model_train\"][\"items_num\"],\n",
    "        embedding_dim=config[\"model_train\"][\"embedding_dim\"],\n",
    "        srm_size=config[\"model_train\"][\"srm_size\"],\n",
    "        state_size=config[\"model_train\"][\"state_size\"],\n",
    "        actor_hidden_dim=config[\"model_train\"][\"actor_hidden_dim\"],\n",
    "        actor_learning_rate=config[\"model_train\"][\"actor_learning_rate\"],\n",
    "        critic_hidden_dim=config[\"model_train\"][\"critic_hidden_dim\"],\n",
    "        critic_learning_rate=config[\"model_train\"][\"critic_learning_rate\"],\n",
    "        discount_factor=config[\"model_train\"][\"discount_factor\"],\n",
    "        tau=config[\"model_train\"][\"tau\"],\n",
    "        learning_starts=len(available_users) + 1,# config[\"model_train\"][\"learning_starts\"],\n",
    "        replay_memory_size=config[\"model_train\"][\"replay_memory_size\"],\n",
    "        batch_size=config[\"model_train\"][\"batch_size\"],\n",
    "        embedding_network_weights_path=\"../{}\".format(config[\"model_train\"][\"embedding_network_weights\"]),\n",
    "        n_groups=config[\"model_train\"][\"n_groups\"],\n",
    "        fairness_constraints=config[\"model_train\"][\"fairness_constraints\"],\n",
    "        use_reward_model=config[\"model_train\"][\"use_reward_model\"],\n",
    "    )\n",
    "\n",
    "    for user_id in tqdm(available_users):\n",
    "\n",
    "        eval_env = ENV[algorithm](\n",
    "            users_dict=dataset[\"eval_users_dict\"],\n",
    "            users_history_lens=dataset[\"eval_users_history_lens\"],\n",
    "            n_groups=config[\"model_train\"][\"n_groups\"],\n",
    "            item_groups=dataset[\"item_groups\"],\n",
    "            state_size=config[\"model_train\"][\"state_size\"],\n",
    "            done_count=k, #config[\"model_train\"][\"done_count\"],\n",
    "            fairness_constraints=config[\"model_train\"][\"fairness_constraints\"],\n",
    "            reward_threshold=config[\"model_train\"][\"reward_threshold\"],\n",
    "            reward_version=config[\"model_train\"][\"reward_version\"],\n",
    "            fix_user_id=user_id,\n",
    "            reward_model=recommender.reward_model,\n",
    "            device=recommender.device,\n",
    "            use_only_reward_model=True,\n",
    "        )\n",
    "\n",
    "        # recommender.env = eval_env\n",
    "\n",
    "        # recommender.buffer = pickle.load(open(os.path.join(output_path, \"buffer.pkl\"), \"rb\"))\n",
    "\n",
    "        precision, ndcg, propfair, reward, list_recommended_item, _, _ = recommender.online_evaluate(\n",
    "            top_k=k, load_model=True, env = eval_env\n",
    "        )\n",
    "\n",
    "        recommended_item.append(list_recommended_item)\n",
    "        random_recommended_item.append({user_id: sample(catalog, k)})\n",
    "\n",
    "        sum_precision += precision\n",
    "        sum_propfair += propfair\n",
    "        sum_reward += reward\n",
    "\n",
    "        del eval_env\n",
    "\n",
    "\n",
    "    _precision.append(sum_precision / len(dataset[\"eval_users_dict\"]))\n",
    "    _propfair.append(sum_propfair / len(dataset[\"eval_users_dict\"]))\n",
    "    _ufg.append((sum_propfair / len(dataset[\"eval_users_dict\"]))\n",
    "        / (1 - (sum_precision / len(dataset[\"eval_users_dict\"]))))\n",
    "    _recommended_item.append(recommended_item)\n",
    "    _random_recommended_item.append(random_recommended_item)\n",
    "\n",
    "\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(item_groups_df[[\"item_id\"]].apply(lambda x: recommender.get_items_emb(x).cpu().numpy().tolist())[\"item_id\"].tolist())\n",
    "\n",
    "metrics = {}\n",
    "for k in range(len(top_k)):\n",
    "    recs = pd.DataFrame([i.values() for i in _recommended_item[k]], columns=[\"sorted_actions\"]).sorted_actions.values.tolist()\n",
    "    #random_recs = pd.DataFrame([i.values() for i in random_recommended_item], columns=[\"sorted_actions\"]).sorted_actions.values.tolist()\n",
    "\n",
    "    metrics[top_k[k]] = {\n",
    "        \"precision\": round(_precision[k] * 100, 4),\n",
    "        \"propfair\": round(_propfair[k] * 100, 4),\n",
    "        \"ufg\": round(_ufg[k], 4),\n",
    "        \"coverage\": round(rm.prediction_coverage(recs, catalog) * 100, 4),\n",
    "        \"personalization\": round(rm.personalization(recs) * 100, 4),\n",
    "        \"intra_list_similarity\": round(rm.intra_list_similarity(recs, feature_df), 4),\n",
    "    }\n",
    "\n",
    "with open(os.path.join(output_path, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in top_k:\n",
    "    _df = pd.DataFrame([i.values() for i in recommended_item], columns=[\"sorted_actions\"])\n",
    "    _df[\"user_id\"] = [list(i.keys())[0] for i in recommended_item]\n",
    "    _item_metadata = pd.DataFrame(dataset[\"item_groups\"].items(), columns=[\"item_id\", \"group\"])\n",
    "\n",
    "    user_column = \"user_id\"\n",
    "    item_column = \"item_id\"\n",
    "    reclist_column = \"sorted_actions\"\n",
    "\n",
    "    recsys_fair = RecsysFair(\n",
    "        df = _df, \n",
    "        supp_metadata = _item_metadata,\n",
    "        user_column = user_column, \n",
    "        item_column = item_column, \n",
    "        reclist_column = reclist_column, \n",
    "    )\n",
    "\n",
    "    fair_column = \"group\"\n",
    "    ex = recsys_fair.exposure(fair_column, k)\n",
    "\n",
    "    fig = ex.show(kind='per_group_norm', column=fair_column)\n",
    "    fig.write_image(os.path.join(output_path, \"exposure_per_group_k{}.png\".format(k)))\n",
    "\n",
    "    fig = ex.show(kind='per_rank_pos', column=fair_column)\n",
    "    fig.write_image(os.path.join(output_path, \"exposure_per_rank_k{}.png\".format(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "RL_ActorCritic_DDPG_Movie_Recommendation.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8842d3533bae659e5f41b6e8512932590b59a54a4b7636cc41bd679d9b5f82e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('recsysrl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
